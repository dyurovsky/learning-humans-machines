<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Assignment details | Learning in Humans and Machines</title>
    <link>/learning-humans-machines/assignment/</link>
      <atom:link href="/learning-humans-machines/assignment/index.xml" rel="self" type="application/rss+xml" />
    <description>Assignment details</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <image>
      <url>/learning-humans-machines/img/social-image.png</url>
      <title>Assignment details</title>
      <link>/learning-humans-machines/assignment/</link>
    </image>
    
    <item>
      <title>Homework 1</title>
      <link>/learning-humans-machines/assignment/01-rescorla-wagner/</link>
      <pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/assignment/01-rescorla-wagner/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-rescorla-wagner-model&#34;&gt;The Rescorla-Wagner Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simple-conditioning&#34;&gt;Simple conditioning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extinction&#34;&gt;Extinction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#blocking&#34;&gt;Blocking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditioned-inhibition&#34;&gt;Conditioned inhibition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;style type=&#34;text/css&#34;&gt;
h4 {
  margin-top: 10px;
  margin-bottom: 10px;
  padding-top: 10px;
  padding-bottom: 10px;
  border-color: #d45026;
  border-style: solid;
  background-color: rgba(212, 80, 38, 0.2);
  font-weight: normal;
}

&lt;/style&gt;
&lt;p&gt;&lt;strong&gt;Getting your assignment: &lt;/strong&gt; You can find template code for your submission here at this &lt;a href=&#34;https://classroom.github.com/a/03b_G9Em&#34;&gt;GitHub Classroom link&lt;/a&gt;. All of the code you write you should go in &lt;code&gt;hw1.Rmd&lt;/code&gt;, and please knit the Markdown file in your completed submission.&lt;/p&gt;
&lt;div id=&#34;the-rescorla-wagner-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Rescorla-Wagner Model&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;../../class/03-class/papers/rescorlawagner1972.pdf&#34;&gt;Rescorla-Wagner model&lt;/a&gt;, developed by Robert Rescorla and Allen Wagner in 1972, was extremely influential at the time of its publication because it was able to explain several puzzling findings in Pavlovian condition, especially the phenomenon of blocking. It has since been extended by researchers working in Reinforcement learning to account for a number of other interesting phenomena. It is also the basis of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Delta_rule&#34;&gt;delta rule&lt;/a&gt; used for training simple neural networks, as you’ll see later on the course.&lt;/p&gt;
&lt;p&gt;You’ll work with the same simplified version of the model that you saw in &lt;a href=&#34;../../class/03-class/slides/associative_learning.pdf&#34;&gt;class&lt;/a&gt;. The model describes the change in strength associated with a conditioned stimulus (&lt;span class=&#34;math inline&#34;&gt;\(\Delta V\)&lt;/span&gt;) with this equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Delta V = \alpha \cdot \left(\lambda - V_{total}\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;problem-1-describe-in-words-what-each-symbol-in-this-equation-means-and-how-it-is-related-to-learning-1-point.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 1&lt;/strong&gt;: Describe, in words, what each symbol in this equation means and how it is related to learning (1 point).&lt;/h4&gt;
&lt;p&gt;Now let’s turn this equation into &lt;code&gt;R&lt;/code&gt; code. You’ll write a function called &lt;code&gt;rw_delta_v&lt;/code&gt; that computes &lt;span class=&#34;math inline&#34;&gt;\(\Delta V\)&lt;/span&gt; according to the Rescorla-Wagner equation. It should take all three of relevant parameters(&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(V_{total}\)&lt;/span&gt;). It should return the amount that the target weight will change.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2-write-rw_delta_v-1-point.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 2&lt;/strong&gt;: Write &lt;code&gt;rw_delta_v&lt;/code&gt; (1 point).&lt;/h4&gt;
&lt;p&gt;You’ll use the stub in the R Markdown file in the GitHub Repository that looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rw_delta_v &amp;lt;- function(Vtotal, alpha = .1, lambda = 1) {
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing to notice is that function parameters in &lt;code&gt;R&lt;/code&gt; can have defaults specified. If you don’t pass in a value for that parameter, it will get the default value inside the function.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-conditioning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple conditioning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/forward_conditioning.png&#34; width=&#34;60%&#34; style=&#34;display: block; margin: auto auto auto 0;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To get started, you’ll simulate of simple conditioning experiment in which someone experiences 10 trials of positive reinforcement in response to a conditioned stimulus. You’ll want to produce a plot of the strength of the conditioned stimulus over the course of these 10 trials so you can see the changes that the model predicts.&lt;/p&gt;
&lt;p&gt;There are lots of ways of setting up this experiment in &lt;code&gt;R&lt;/code&gt;, and you’re welcome to do it however you like. In case you’d like a hand to get started, he’s one strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Make a &lt;code&gt;tibble&lt;/code&gt; with 2 columns: &lt;code&gt;trial&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt;. The &lt;code&gt;trial&lt;/code&gt; column will have the values &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;, and the &lt;code&gt;V&lt;/code&gt; column will start as all &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;s.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a for loop that iterates over the numbers &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;—these will index into the rows of your &lt;code&gt;tibble&lt;/code&gt;. Set the value of the &lt;code&gt;V&lt;/code&gt; column in each row to the result of calling your &lt;code&gt;rw_delta_v&lt;/code&gt; function on the value of &lt;code&gt;V&lt;/code&gt; in the previous row.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make a plot with &lt;code&gt;trial&lt;/code&gt; on the x-axis and &lt;code&gt;V&lt;/code&gt; on the y-axis. You can make one plot for all four of your simulations if you’re feeling comfortable with the &lt;code&gt;tidyverse&lt;/code&gt;, or 4 separate plots.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-3-run-4-simulations-of-the-experimenttry-2-different-levels-of-alpha-and-2-different-levels-of-lambda.-what-effect-does-these-parameters-have-on-the-model-predictions-3-points.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 3&lt;/strong&gt;: Run 4 simulations of the experiment–try 2 different levels of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and 2 different levels of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. What effect does these parameters have on the model predictions? (3 points).&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;extinction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extinction&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/extinction.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Now let’s see what happens if we take the reinforcer away. Set up a simulation where the participant is exposed to 10 trials in of positive reinforcement in response to a conditioned stimulus, and then 10 trials in which they get no reinforcement (&lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0\)&lt;/span&gt;). Then make a plot of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; over the course of the experiment.&lt;/p&gt;
&lt;div id=&#34;problem-4-try-the-same-parameter-values-that-you-used-above-in-this-new-experiment.-how-does-the-extinction-curve-depend-on-alpha-and-lambda-2-points.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 4&lt;/strong&gt;: Try the same parameter values that you used above in this new experiment. How does the extinction curve depend on &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;? (2 points).&lt;/h4&gt;
&lt;p&gt;This should be a fairly straightforward extension of the code you wrote for the last Problem. The critical thing will be to make sure that you are using the right value of the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; parameter on each trial—remember, no reinforcer should have no reward and thus &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;blocking&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Blocking&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/blocking.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Now you’re ready to test Rescorla-Wagner’s ability to account for the Blocking phenomenon. In this simulation, you’ll have two cues–&lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. For the first 10 trials, the simulated participant will be exposed to cue &lt;code&gt;x&lt;/code&gt; and be positively reinforced. Then, on the subsequent 30 trials, both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; will be present and the participant will get reinforced.&lt;/p&gt;
&lt;p&gt;One way to make this simulation work is to replace the &lt;code&gt;V&lt;/code&gt; column with two new columns: &lt;code&gt;Vx&lt;/code&gt; and &lt;code&gt;Vy&lt;/code&gt;. And then include two more columns in your tibble, &lt;code&gt;x_present&lt;/code&gt; and &lt;code&gt;y_present&lt;/code&gt;, which indicate whether each cue is present on each trial. You then want to make sure that you simulate updating the weight for all cues that are present &lt;span class=&#34;math inline&#34;&gt;\(\Delta V\)&lt;/span&gt;. And make sure that &lt;span class=&#34;math inline&#34;&gt;\(V_{total}\)&lt;/span&gt; has the right value on each trial!&lt;/p&gt;
&lt;div id=&#34;problem-5-implement-the-blocking-experiment-described-above-and-make-a-plot-of-the-weights-of-each-of-the-two-cues-over-the-course-of-the-40-trials.-you-can-pick-any-values-for-alpha-and-lambda-3-points.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 5&lt;/strong&gt;: Implement the blocking experiment described above and make a plot of the weights of each of the two cues over the course of the 40 trials. You can pick any values for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (3 points).&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditioned-inhibition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditioned inhibition&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/conditioned_inhibition.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Finally, you’re ready to simulate a phenomenon we talked about in class but that you didn’t see directly: Conditioned inhibition. The setup is similar to blocking–first one cue (&lt;code&gt;x&lt;/code&gt;) is presented and reinforced, and then &lt;code&gt;x&lt;/code&gt; and another cue &lt;code&gt;y&lt;/code&gt; appear together. But, this time their combination is &lt;strong&gt;not&lt;/strong&gt; reinforced. As a result, people (and the model) learn a negative value for &lt;code&gt;y&lt;/code&gt;. Intuitively, &lt;code&gt;y&lt;/code&gt; is the reason that &lt;code&gt;x&lt;/code&gt; appearing did not lead to a positive reinforcer.&lt;/p&gt;
&lt;div id=&#34;problem-6-implement-the-a-conditioned-inhibition-experiment-just-like-your-blocking-experiment-above-except-this-time-without-reinforcement-on-the-30-trials-with-both-stimuli.-make-a-plot-of-the-values-of-the-two-cues-over-the-course-of-the-experiment.-pick-any-values-for-alpha-and-lambda-2-points.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 6&lt;/strong&gt;: Implement the a conditioned inhibition experiment just like your blocking experiment above, except this time without reinforcement on the 30 trials with both stimuli. Make a plot of the values of the two cues over the course of the experiment. Pick any values for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (2 points).&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Homework 2</title>
      <link>/learning-humans-machines/assignment/02-networks/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/assignment/02-networks/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#perceptrons&#34;&gt;Perceptrons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-layer-networks&#34;&gt;Multi-layer networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-the-neuralnet-package&#34;&gt;Using the &lt;code&gt;neuralnet&lt;/code&gt; package&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;style type=&#34;text/css&#34;&gt;
h4 {
  margin-top: 10px;
  margin-bottom: 10px;
  padding-top: 10px;
  padding-bottom: 10px;
  border-color: #d45026;
  border-style: solid;
  background-color: rgba(212, 80, 38, 0.2);
  font-weight: normal;
}

&lt;/style&gt;
&lt;p&gt;&lt;strong&gt;Getting your assignment: &lt;/strong&gt; You can find template code for your submission here at this &lt;a href=&#34;https://classroom.github.com/a/UoRs801q&#34;&gt;GitHub Classroom link&lt;/a&gt;. All of the code you write you should go in &lt;code&gt;hw2.Rmd&lt;/code&gt;, and please knit the Markdown file in your completed submission.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Neural networks were a revolution in the scientific study of cognitive neuroscience, spawning a large body of work investigating the computational properties of systems designed to model the operation of the brain’s basic computational units (&lt;a href=&#34;https://dyurovsky.github.io/learning-humans-machines/class/05-class/papers/rosenblatt1958.pdf&#34;&gt;Rosenblatt, 1958&lt;/a&gt;. These perceptrons varied along a number of dimensions, but all of them had the same critical flaw: they could not learn non-linear combinations of inputs, leading to their failure on even simple problems like Exclusive Or (XOR; Minsky &amp;amp; Papert, 1967).&lt;/p&gt;
&lt;p&gt;In the 1980s, work by David Rumelhart and his colleagues rekindled the field’s interest in neural networks by devising an algorithm by which these models &lt;em&gt;could&lt;/em&gt; learn non-linear combinations of input and develop genuinely interesting and surprisingly representations of their input (&lt;a href=&#34;https://dyurovsky.github.io/learning-humans-machines/class/06-class/papers/rumelhart1986.pdf&#34;&gt;Rumelhart, Hinton, &amp;amp; Williams, 1986&lt;/a&gt;). This lead to an explosion of work in artificial neural networks in the following decades, and also, following Marr’s perspective, a reconsideration of what these models were intended to describe (e.g. the neurons in the networks need not map on to neurons in the brain).&lt;/p&gt;
&lt;p&gt;In this assignment you will first implement simple one-layer perceptrons that learn with the perceptron learning rule. You’ll show that these can learn several logical functions: AND, OR, and NOT. But they cannot learn XOR.&lt;/p&gt;
&lt;p&gt;You will then build a very simple multi-layer network that uses backpropagation to learn XOR. You’ll finally use the &lt;code&gt;neuralnet&lt;/code&gt; package to solve this same problem, and then use it to build a digit classifier using a small version of the &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;MNIST dataset&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;perceptrons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Perceptrons&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/perceptron.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Your first goal will be to train a perceptron to solve logical AND. I’ve provided a set of stub functions that scaffold one way of doing this. The idea is to approximate a sort of pseudo-object oriented structure using a named list. This is overkill for just this simple perceptron, but you’ll find that it extends easily to a backprop network.&lt;/p&gt;
&lt;p&gt;This object defined in the &lt;code&gt;perceptron&lt;/code&gt; function. A &lt;code&gt;perceptron&lt;/code&gt; is a list that has 5 elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A &lt;code&gt;tibble&lt;/code&gt; of inputs&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A list of target &lt;code&gt;y&lt;/code&gt;s for those inputs&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A list of &lt;code&gt;output&lt;/code&gt;s for the last run of the network corresponding to these inputs&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A list of &lt;code&gt;activation&lt;/code&gt;s that occur after applying the &lt;code&gt;sigmoid&lt;/code&gt; function to those &lt;code&gt;output&lt;/code&gt;s&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A list of &lt;code&gt;weights&lt;/code&gt; – 3 in total. Two that connect from the input nodes (&lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;), and one that connects from a bias node (&lt;code&gt;1&lt;/code&gt;) to the output node (&lt;code&gt;y&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This list will track the state of the perceptron as it goes through the training function you write (&lt;code&gt;train_perceptron&lt;/code&gt;). You can write this function as two nested for loop, the outer one over iterations, and the inner one over examples. In each run of the inner loop, you will&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;perceptron_feedforward&lt;/code&gt; over the training example.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;perceptron_feedback&lt;/code&gt; over that training example to update the weights.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In each run of the outer loop, you will run &lt;code&gt;perceptron_feedforward&lt;/code&gt; and &lt;code&gt;perceptron_feewdback&lt;/code&gt; on all of the examples, and then compute the error for that iteration and store it in the network, e.g. &lt;code&gt;perceptron$errors[iteration] &amp;lt;- sq_error(...)&lt;/code&gt;.You’ll need to make sure you define &lt;code&gt;perceptron$errors&lt;/code&gt; as a list of the right length at the start of the &lt;code&gt;train_perceptron&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;If you want to use these stubs, your plan of attack should be:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write the helper functions &lt;code&gt;sigmoid&lt;/code&gt; and &lt;code&gt;sq_error&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make the &lt;code&gt;and_data&lt;/code&gt; tibble with columns &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; that corresponds to the four possible input and output combinations for AND&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write the &lt;code&gt;perceptron_feedforward&lt;/code&gt; and &lt;code&gt;perceptron_feedback&lt;/code&gt; functions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write &lt;code&gt;train_perceptron&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;train_perceptron&lt;/code&gt; on your AND data, and plot or otherwise summarize the change in errors over time and the final weights.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;problem-1-fill-out-the-stub-helper-functions-sigmoid-and-sq_error-1-point&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 1&lt;/strong&gt;: Fill out the stub helper functions &lt;code&gt;sigmoid&lt;/code&gt; and &lt;code&gt;sq_error&lt;/code&gt; (1 point)&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2-write-code-to-train-a-perceptron-to-solve-logical-and.-you-can-use-the-stubs-provided-in-the-hw2.rmd-or-write-your-own.-train-the-perceptron-and-report-back-on-the-results.-did-the-error-in-the-network-go-down-over-the-course-of-training-what-were-the-final-weights-3-points&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 2&lt;/strong&gt;: Write code to train a perceptron to solve logical AND. You can use the stubs provided in the &lt;code&gt;hw2.Rmd&lt;/code&gt; or write your own. Train the perceptron, and report back on the results. Did the error in the network go down over the course of training? What were the final weights? (3 points)&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3-use-your-functions-to-train-the-perceptron-on-or-not-x1-and-xor.-which-problems-did-it-succeed-on.-which-did-it-fail-on-what-were-the-final-weights-for-each-do-they-make-sense-2-points&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 3&lt;/strong&gt;: Use your functions to train the perceptron on OR, NOT X1, and XOR. Which problems did it succeed on. Which did it fail on? What were the final weights for each? Do they make sense? (2 points)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-layer-networks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multi-layer networks&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/multi-layer.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;If everything went right, you will have discovered in &lt;strong&gt;Problem 3&lt;/strong&gt; that perceptrons cannot learn to solve non-linear classification problems like XOR. But with a hidden layer, we can fix this problem. You can use the same strategy you used for your perceptron to implement a 2-layer backpropagation network. You might find the backprop by hand example we did helpful for reference for the weight update formulas.&lt;/p&gt;
&lt;div id=&#34;problem-4-write-code-to-train-a-2-layer-network-to-learn-xor.-you-should-have-3-hidden-layer-nodes-2-that-take-input-from-the-input-layer-and-1-bias-node.-after-training-investigate-the-hidden-layer-nodes.-what-has-the-network-learned-3-points&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 4&lt;/strong&gt;: Write code to train a 2-layer network to learn XOR. You should have 3 hidden layer nodes — 2 that take input from the input layer, and 1 bias node. After training, investigate the hidden layer nodes. What has the network learned? (3 points)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-neuralnet-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the &lt;code&gt;neuralnet&lt;/code&gt; package&lt;/h2&gt;
&lt;p&gt;The network you implemented is likely to be pretty inefficient. In practice, most implementations of neural networks use matrix multiplication to compute weights and outputs, which drastically speeds things up. For the last two problems, you’ll be using the &lt;code&gt;neuralnet&lt;/code&gt; r package to investigate what these networks can learn in a more interesting problem.&lt;/p&gt;
&lt;p&gt;The workhorse of the package is the &lt;code&gt;neuralnet&lt;/code&gt; function which trains neural nets to solve problems. It uses formula notation just like &lt;code&gt;lm&lt;/code&gt; and other standard statistical methods in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;problem-5-use-the-provided-code-to-run-neuralnet-on-your-xor-data-and-the-plot-your-neuralnet-to-see-what-the-weights-on-each-node-are.-did-it-learn-the-same-thing-as-the-network-you-made-from-scratch&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 5&lt;/strong&gt;: Use the provided code to run &lt;code&gt;neuralnet&lt;/code&gt; on your xor data, and the &lt;code&gt;plot&lt;/code&gt; your neuralnet to see what the weights on each node are. Did it learn the same thing as the network you made from scratch?&lt;/h4&gt;
&lt;p&gt;Now that you understand how this package works, you’ll use it to solve a more interesting classification problem. You’ll be learning to classify handwritten digits from the &lt;strong&gt;MNIST&lt;/strong&gt; dataset that are a classic success story for modern neural networks. You’ll be working with just a small, scaled down subset of the real dataset — 100 examples of each of the digits 0-9. Here is the first example of each:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/mnist.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Your goal will be to try out the &lt;code&gt;neuralnet&lt;/code&gt; package to find what kinds of network structures matter for learning to classify these digits. The representation you’ll work with is a linearized version of the digits — imagine taking all of the rows of these figures and chaining them all into one long row. This means your network won’t have any ability to use the spatial structure of the images. But even with just the pixel values, you’ll find that you can do a fair bit better than chance (&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{10}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;neuralnet&lt;/code&gt; package uses what is called a &lt;em&gt;one-hot&lt;/em&gt; encoding: The output layer will have ten nodes, each corresponding to a digit. The goal of the network when it reads say a &lt;span class=&#34;math inline&#34;&gt;\(9\)&lt;/span&gt; is for all of the output nodes except for the one corresponding to &lt;span class=&#34;math inline&#34;&gt;\(9\)&lt;/span&gt; to have no activation, and for the &lt;span class=&#34;math inline&#34;&gt;\(9\)&lt;/span&gt; node to have 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-6-play-around-with-the-hidden-layer-structure-of-the-network.-you-can-specify-how-many-units-there-are.-for-instance-using-the-argument-hidden-2-makes-2-hidden-layer-nodes-plus-a-bias.-using-the-argument-hidden-c32-makes-2-hidden-layers---the-first-having-3-units-and-the-second-having-2-plus-a-bias.-if-the-number-of-units-gets-too-big-youll-find-that-the-network-crashes-or-fails-to-converge.-but-report-back-on-at-least-3-argument-values-that-successfully-ran.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 6&lt;/strong&gt;: Play around with the hidden layer structure of the network. You can specify how many units there are. For instance, using the argument &lt;code&gt;hidden = 2&lt;/code&gt; makes 2 hidden layer nodes (plus a bias). using the argument &lt;code&gt;hidden = c(3,2)&lt;/code&gt; makes 2 hidden layers - the first having 3 units and the second having 2 (plus a bias). If the number of units gets too big you’ll find that the network crashes or fails to converge. But report back on at least 3 argument values that successfully ran.&lt;/h4&gt;
&lt;p&gt;You can use the provided &lt;code&gt;prediction_error&lt;/code&gt; function will compute how much error there is. Make sure you look at error both on the training set (&lt;code&gt;mnist_train&lt;/code&gt;) and on a set of examples that it hasn’t been trained on (&lt;code&gt;mnist_test&lt;/code&gt;). Is the network overfitting? What is it learning? Feel free to extend the &lt;code&gt;prediction_error&lt;/code&gt; function to compute other metrics if you think they would be interesting.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Homework 3</title>
      <link>/learning-humans-machines/assignment/03-bayes/</link>
      <pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/assignment/03-bayes/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling-the-number-game&#34;&gt;Modeling the number game&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-candidate-hypotheses&#34;&gt;The candidate hypotheses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling-inference&#34;&gt;Modeling Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inferring-hypotheses&#34;&gt;Inferring hypotheses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predicting-the-next-number.&#34;&gt;Predicting the next number.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;style type=&#34;text/css&#34;&gt;
h4 {
  margin-top: 10px;
  margin-bottom: 10px;
  padding-top: 10px;
  padding-bottom: 10px;
  border-color: #d45026;
  border-style: solid;
  background-color: rgba(212, 80, 38, 0.2);
  font-weight: normal;
}

&lt;/style&gt;
&lt;div id=&#34;template&#34;&gt;
&lt;p&gt;&lt;strong&gt;Getting your assignment: &lt;/strong&gt; You can find template code for your submission here at this &lt;a href=&#34;https://classroom.github.com/a/Z-ph3MLv&#34;&gt;GitHub Classroom link&lt;/a&gt;. All of the code you write you should go in &lt;code&gt;hw3.Rmd&lt;/code&gt;, and please knit the Markdown file in your completed submission.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;An important problem in inferential learning is that the data available are almost always too sparse to unambiguously pick out the true hypothesis (see e.g. Quine, 1960). Human learners, however, appear to cope with this problem just fine, converging on a small set of possible hypothesis from very little data. In machine learning, this is known as few-shot learning, and is a real challenge for modern learning systems. One hypothesis for what helps human learners is that have a strong prior on the kinds of hypotheses that are likely and relevant, which makes the search problem much easier.&lt;/p&gt;
&lt;p&gt;In this assignment, you’ll be working with a simple problem designed to demonstrate this phenomenon, and building a model that learns in a human-like way. The number game, developed by &lt;a href=&#34;https://dyurovsky.github.io/learning-humans-machines/class/11-class/papers/tenenbaum200.pdf&#34;&gt;Josh Tenenbaum (2000)&lt;/a&gt;, demonstrates the power of strong sampling for inductive inference. The key insight is that the probability that data is generated by a hypothesized process is proportional to the size of the set of possible data that the hypothesis could generate.&lt;/p&gt;
&lt;p&gt;In the number game, you get as input one or more numbers between 1 and 100 generated by a computer program according to an unknown rule. Your job is to determine the most likely rule, or to make predictions about which numbers the program will generate next.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/number_game.jpeg&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-the-number-game&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling the number game&lt;/h1&gt;
&lt;p&gt;To figure out what hypothesis the computer is generating samples from, you can use Bayesian inference. To do this, you will use Bayes Rule to infer the posterior probability for each hypothesis:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P\left(H \vert D\right) \propto P\left(D \vert H\right)P\left(H\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To specify the model, you need to specify a prior probability (&lt;span class=&#34;math inline&#34;&gt;\(P\left(H\right)\)&lt;/span&gt;) for each hypothesis, and a likelihood function that specified how probable the observed data are under that hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(P\left(D \vert H\right)\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prior&lt;/strong&gt;: Following Tenenbaum (2000), we’ll say that hypotheses can be of two kinds: Mathematical and Interval. We’ll say that aprior, there is a &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; probability that the true hypothesis is in the set of Mathemtical hypotheses, and a (1-&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;) probability that the true hypothesis is in the set of Interval Hypotheses&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt;: If we assume that the every number consistent with a particular hypothesis is equally likely to be generated by the machine, then if every number in &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;, the likelihood that set of numbers &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is generated by the hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; is the probability of choosing &lt;span class=&#34;math inline&#34;&gt;\(\lvert D \rvert\)&lt;/span&gt; numbers at random from the set of &lt;span class=&#34;math inline&#34;&gt;\(\lvert H \rvert\)&lt;/span&gt; numbers (&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\lvert H \rvert \choose \lvert D \rvert}\)&lt;/span&gt;). You can compute this in R using the &lt;code&gt;choose&lt;/code&gt; function. Otherwise, &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; cannot generate &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; and the likelihood is 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-candidate-hypotheses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The candidate hypotheses&lt;/h1&gt;
&lt;p&gt;Your first goal is to write all of the candidate hypotheses that your model will consider. You don’t need to exhaustively include all of the hypotheses in the original Tenenbaum (2000) model, just the following&lt;/p&gt;
&lt;p&gt;Mathematical hypotheses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;even numbers&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;odd numbers&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;square numbers&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;cube numbers&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;prime numbers&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;multiples of any number &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\left(3 \leq n \leq 12\right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;powers of any number &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\left(2 \leq n \leq 10\right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interval hypotheses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;decades (&lt;span class=&#34;math inline&#34;&gt;\(\left\{1-10,\; 10-20, \;\ldots \right\}\)&lt;/span&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;any range &lt;span class=&#34;math inline&#34;&gt;\(1 \leq n \leq 100, n \leq m \leq 100, \left\{n-m\right\}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One nice way of representing these hypotheses as &lt;code&gt;tibble&lt;/code&gt;s with two columns: &lt;code&gt;name&lt;/code&gt; which gives a human-readable description of the hypothesis, and &lt;code&gt;set&lt;/code&gt; which is a &lt;code&gt;list&lt;/code&gt; of all of the numbers 1—100 consistent with that hypothesis. You’ll first. write a function that generates each of these. For the multiple-set hypotheses (e.g. multiples of any number), you can make the function generate a multi-row &lt;code&gt;tibble&lt;/code&gt; where each row is one of the sub-hypotheses.&lt;/p&gt;
&lt;div id=&#34;problem-1-define-all-of-the-hypotheses-for-the-number-game.-youre-welcome-to-use-the-stubs-i-have-provided-in-the-template.-3-points.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 1&lt;/strong&gt;: Define all of the hypotheses for the number game. You’re welcome to use the stubs I have provided in the template. (3 points).&lt;/h4&gt;
&lt;p&gt;Now that you have each of the individual hypotheses, you can put them all together. A nice representation for this is one big tibble that has a row for all of the hypotheses you’ve generated. You’ll find the &lt;code&gt;bind_rows&lt;/code&gt; function to be helpful here.&lt;/p&gt;
&lt;p&gt;The last detail is that you want to be able to assign a prior probability appropriately to each of these hypotheses. Remember, all of the mathematical hypothesis together should have prior probability &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, and the interval hypotheses should make up the remainder of the prior (&lt;span class=&#34;math inline&#34;&gt;\(1 - \lambda\)&lt;/span&gt;). You’ll want to divy up this probability equally within those respective sets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2-write-the-function-make_all_hypotheses-which-takes-in-a-value-for-the-parameter-lambda-and-construct-the-full-tibble-of-hypotheses.-it-should-have-four-columns-name-set-type-mathematical-or-interval-and-prior-which-have-a-value-of-each-of-all-of-the-possible-hypotheses.-hint-there-should-be-5084-total-1-point.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 2&lt;/strong&gt;: Write the function &lt;code&gt;make_all_hypotheses&lt;/code&gt; which takes in a value for the parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and construct the full tibble of hypotheses. It should have four columns: &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;set&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt; (mathematical or interval), and &lt;code&gt;prior&lt;/code&gt; which have a value of each of all of the possible hypotheses. Hint: There should be 5,084 total (1 point).&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-inference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling Inference&lt;/h1&gt;
&lt;p&gt;Now that you have all of your hypotheses, you need to write the likelihood function. And finally to compute the posterior probability of each hypothesis.&lt;/p&gt;
&lt;div id=&#34;problem-3-write-the-function-compute_likelihood-which-takes-in-a-set-of-numbers-consistent-with-a-hypothesis-h-and-a-set-of-input-numbers-d-and-which-returns-the-likelihood-of-generating-that-input-set-from-that-hypothesized-number-set-pleftd-vert-hright-1-point.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 3&lt;/strong&gt;: Write the function &lt;code&gt;compute_likelihood&lt;/code&gt; which takes in a set of numbers consistent with a hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; , and a set of input numbers &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, and which returns the likelihood of generating that input set from that hypothesized number set (&lt;span class=&#34;math inline&#34;&gt;\(P\left(D \vert H\right)\)&lt;/span&gt;) (1 point).&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4-write-the-function-best_hypotheses-which-takes-in-a-tibble-of-hypotheses-h-a-set-of-input-numbers-d-an-optional-number-of-hypotheses-n-and-returns-a-tibble-with-the-top-n-hypotheses-for-the-data-according-to-the-posterior-probability-plefth-vert-dright-as-well-as-their-corresponding-posterior-probability.-you-might-find-it-helpful-not-to-return-every-row-of-the-original-hypotheses-tibble-but-only-their-type-mathematical-or-interval-name-and-posterior-2-points.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 4&lt;/strong&gt;: Write the function &lt;code&gt;best_hypotheses&lt;/code&gt; which takes in a &lt;code&gt;tibble&lt;/code&gt; of hypotheses (&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;), a set of input numbers &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, an optional number of hypotheses &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, and returns a &lt;code&gt;tibble&lt;/code&gt; with the top &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; hypotheses for the data according to the posterior probability (&lt;span class=&#34;math inline&#34;&gt;\(P\left(H \vert D\right)\)&lt;/span&gt;) as well as their corresponding posterior probability. You might find it helpful not to return every row of the original hypotheses tibble, but only their &lt;code&gt;type&lt;/code&gt; (mathematical or interval), &lt;code&gt;name&lt;/code&gt;, and &lt;code&gt;posterior&lt;/code&gt; (2 points).&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;inferring-hypotheses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Inferring hypotheses&lt;/h1&gt;
&lt;p&gt;Now you are ready to test the model! Using the default &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; value of &lt;span class=&#34;math inline&#34;&gt;\(\frac{4}{5}\)&lt;/span&gt;, you will get the top 5 best hypotheses for each of the following sets of data &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(2, 4, 8\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(15, 22\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(1, 2, 3, 4, 6\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-5-print-out-the-top-5-best-hypotheses-according-to-the-model-for-each-of-these-values.-does-the-model-predict-what-you-think-it-should-were-there-any-surprises-2-points&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 5&lt;/strong&gt;: Print out the top 5 best hypotheses according to the model for each of these values. Does the model predict what you think it should? Were there any surprises? (2 points)&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-6-try-changing-the-value-of-lambda-to-something-smaller-like-frac15.-what-effect-does-this-have-on-the-models-predictions-for-the-examples-above-1-point&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 6&lt;/strong&gt;: Try changing the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; to something smaller, like &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{5}\)&lt;/span&gt;. What effect does this have on the model’s predictions for the examples above? (1 point)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-the-next-number.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Predicting the next number.&lt;/h1&gt;
&lt;p&gt;Now you will implement the final part of the model: Prediction about whether the computer is likely to generate a new number &lt;span class=&#34;math inline&#34;&gt;\(d&amp;#39;\)&lt;/span&gt; having observed all of the previous examples &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;. To do this, you will use Bayesian model averaging. Instead of committing to the best hypothesis after observing &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, you will average together all of the hypotheses, weighing them by the posterior probability of them being true after observing &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P\left(d&amp;#39;\right) = \sum_{h \in H}P\left(d&amp;#39; | h\right)P\left(h | D\right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You’ll just need to write one new function &lt;code&gt;predict_value&lt;/code&gt; which will take in a &lt;code&gt;tibble&lt;/code&gt; of hypotheses, a set of previously observed numbers, and a new target number. It should return the posterior probability of observing that number according to the formula above. You shouldn’t need any new helper functions here!&lt;/p&gt;
&lt;p&gt;To check whether it works correctly, try these examples using the default value for &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D = 2, \;d&amp;#39; = 12\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D = 2, 4, 8, \;d&amp;#39; = 12\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D = 15, 22 \; d&amp;#39; = 21\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(D = 15, 22 \; d&amp;#39; = 50\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-7-write-the-predict_value-function-and-test-it-on-the-examples-above.-did-you-get-the-output-you-expected-2-points&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 7&lt;/strong&gt;: Write the &lt;code&gt;predict_value&lt;/code&gt; function and test it on the examples above. Did you get the output you expected? (2 points)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Homework 4</title>
      <link>/learning-humans-machines/assignment/04-mcmc/</link>
      <pubDate>Mon, 26 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/assignment/04-mcmc/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-metropolis-algorithm&#34;&gt;The Metropolis Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sampling-from-an-exponential-distribution&#34;&gt;Sampling from an Exponential Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sampling-from-a-dirichlet-distribution&#34;&gt;Sampling from a Dirichlet Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inference-via-markov-chain-monte-carlo&#34;&gt;Inference via Markov Chain Monte Carlo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;style type=&#34;text/css&#34;&gt;
h4 {
  margin-top: 10px;
  margin-bottom: 10px;
  padding-top: 10px;
  padding-bottom: 10px;
  border-color: #d45026;
  border-style: solid;
  background-color: rgba(212, 80, 38, 0.2);
  font-weight: normal;
}

&lt;/style&gt;
&lt;div id=&#34;template&#34;&gt;
&lt;p&gt;&lt;strong&gt;Getting your assignment: &lt;/strong&gt; You can find template code for your submission here at this &lt;a href=&#34;https://classroom.github.com/a/2w7lj1Y3&#34;&gt;GitHub Classroom link&lt;/a&gt;. All of the code you write you should go in &lt;code&gt;hw4.Rmd&lt;/code&gt;, and please knit the Markdown file in your completed submission.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the last decade, we have seen an explosion of impressive results in Machine Learning, producing models that match and even sometimes outperform humans on &lt;a href=&#34;https://dyurovsky.github.io/learning-humans-machines/reading/17-reading/lake2017.pdf&#34;&gt;challenging tasks&lt;/a&gt;. A non-trivial part of the challenge in developing these models is not in specifying how they should solve the problems they are developed solve, but in figuring out how to actually derive predictions from these models.&lt;/p&gt;
&lt;p&gt;Earlier this semester, we read a paper from &lt;a href=&#34;https://dyurovsky.github.io/learning-humans-machines/reading/12-reading/kemp2007.pdf&#34;&gt;Charles Kemp, And Perfors, and Josh Tenebaum&lt;/a&gt; that shows how overhypotheses like the shape bias can be understood through the lens of rational analysis. Because the model does not have an analytic solution, A sizeable chunk of the article describes approximation methods that the authors use for for determining approximately what the model predicts.&lt;/p&gt;
&lt;p&gt;The approach that Kemp et al. use, and that is a mainstay in modern Bayesian approaches, is technique called Markov chain Monte Carlo (MCMC). Markov chain Monte Carlo is a method for approximating samples from a complex Distribution by drawing samples from other simpler distributions and reweighing or combining them together.&lt;/p&gt;
&lt;p&gt;In this assignment, you will implement the Metropolis Algorithm, an MCMC algorithm in the sequential sampling family. In this family of algorithms, samples are drawn from a simple Proposal distribution and then either Accepted (if they have high probability under the target distribution), or Rejected in favor of keep the current sample.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;the-metropolis-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Metropolis Algorithm&lt;/h2&gt;
&lt;p&gt;Metropolis is a simple sequential sampling scheme for drawing samples from a target distribution &lt;span class=&#34;math inline&#34;&gt;\(P\left(x\right)\)&lt;/span&gt; by using a density function &lt;span class=&#34;math inline&#34;&gt;\(f\left(x\right)\)&lt;/span&gt; that is proportional to &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;– i.e., that can give the relative probability of two different samples &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The algorithm works by starting a chain at some arbitrary point &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;. Then, in each iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, a new sample &lt;span class=&#34;math inline&#34;&gt;\(x1\)&lt;/span&gt; by modifying the previous sample &lt;span class=&#34;math inline&#34;&gt;\(x_{t}\)&lt;/span&gt; according to a proposal distribution &lt;span class=&#34;math inline&#34;&gt;\(g\left(x&amp;#39;|x_{t}\right)\)&lt;/span&gt;. The critical requirement is that &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is a symmetric distribution &lt;span class=&#34;math inline&#34;&gt;\(g\left(x&amp;#39;|x\right) = g\left(x|x&amp;#39;\right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For your simulation, you should use a simple Normal distribution centered at the previous sample &lt;span class=&#34;math inline&#34;&gt;\(g\left(x&amp;#39;|x\right) \sim \text{Normal}\left(x, \sigma\right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;You then run the algorithm for some number of iterations, e.g. 1000, at each iteration performing the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Propose a new sample &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt; by drawing from &lt;span class=&#34;math inline&#34;&gt;\(g\left(x&amp;#39;|x_{t}\right)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the acceptance ratio &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \frac{f\left(x&amp;#39;\right)}{f\left(x\right)}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sample &lt;span class=&#34;math inline&#34;&gt;\(u \sim \text{Uniform}\left(0,1\right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(u \leq \alpha\)&lt;/span&gt;, accept the proposed sample (&lt;span class=&#34;math inline&#34;&gt;\(x_{t+1} = x&amp;#39;\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Otherwise, reject the proposed sample and keep the previous sample (&lt;span class=&#34;math inline&#34;&gt;\(x_{t+1} = x_{t}\)&lt;/span&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-from-an-exponential-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sampling from an Exponential Distribution&lt;/h2&gt;
&lt;p&gt;Exponential distributions are a common distribution in models of cognition, for example they are sometimes used to model forgetting in memory. The Exponential distribution has one parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; which defines the rate of decay (or forgetting).&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(x \sim \text{Exponential}\left(\lambda\right)\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(P\left(x\right) = \lambda e^{-\lambda x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt; actually can draw samples from an Exponential distribution. For example, here are 1000 samples from &lt;span class=&#34;math inline&#34;&gt;\(\text{Exponential}\left(3\right)\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;samples &amp;lt;- tibble(sample = 1:1000, value = rexp(1000, rate = 3))

ggplot(samples, aes(x = value)) + 
  geom_histogram(fill = &amp;quot;white&amp;quot;, color = &amp;quot;black&amp;quot;)b&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/exponential3.jpeg&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;But let’s see how can draw samples from an exponential distribution using Metropolis-Hastings samples instead.&lt;/p&gt;
&lt;p&gt;You will need to do 4 things:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Define the Exponential target distribution &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; using the Exponential definition above.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define the proposal distribution &lt;span class=&#34;math inline&#34;&gt;\(g(x&amp;#39;|x)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define and Metropolis sequential sampling algorithm&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the Algorithm for some number of iterations and plot the results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;problem-1-implement-the-exponential-probability-density-function.-this-function-should-take-in-a-value-x-and-a-parameter-lambda-and-return-the-probability-of-sampling-the-value-x-from-the-exponential-distribution-with-parameter-lambda-see-above-1-point.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 1&lt;/strong&gt;: Implement the Exponential probability density function. This function should take in a value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and return the probability of sampling the value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from the Exponential distribution with parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (see above, 1 point).&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2-implement-a-gaussian-proposal-function.-this-should-take-in-a-current-value-x-a-parameter-sigma-which-defines-how-wide-the-proposal-distribution-is.-it-should-return-a-new-proposed-sample-x-1-point.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 2&lt;/strong&gt;: Implement a Gaussian proposal function. This should take in a current value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, a parameter sigma which defines how wide the proposal distribution is. It should return a new proposed sample &lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt; (1 point).&lt;/h4&gt;
&lt;p&gt;One thing you might notice is that the Metropolis samples won’t look like Exponential samples right away. Here is a plot of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; for 1000 samples from Metropolis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(output, aes(x = iteration, y = f)) + 
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/learning-humans-machines/learning-humans-machines/img/assignments/burnin.jpeg&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;You can see that first several hundred samples, every sample has low probability under the target distribution. This happens when the starting point of the chain is far away from the area of high density in the target distribution. For this reason, it is common to discard some of the initial samples–a process called &lt;strong&gt;burn in&lt;/strong&gt;. You may find this helpful in future simulations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3-implement-a-metropolis-sampler-for-an-exponential-distribution-and-use-it-draw-1000-samples-from-an-exponential-distribution-with-lambda3.-try-lambda1-as-well.-do-the-distributions-look-different-you-should-use-the-two-functions-you-just-wrote-for-problems-1-and-2.-2-points&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 3&lt;/strong&gt;: Implement a Metropolis sampler for an Exponential distribution and use it draw 1000 samples from an Exponential distribution with &lt;span class=&#34;math inline&#34;&gt;\(\lambda=3\)&lt;/span&gt;. Try &lt;span class=&#34;math inline&#34;&gt;\(\lambda=1\)&lt;/span&gt; as well. Do the distributions look different? You should use the two functions you just wrote for Problems 1 and 2. (2 points)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-from-a-dirichlet-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sampling from a Dirichlet Distribution&lt;/h2&gt;
&lt;p&gt;The Dirichlet Distribution is a highly flexible distribution that is that can many different forms depending on its parameters. It often finds use in Cognitive models precisely when the goal is to learn about something than has unknown shape before learning starts (e.g. the probability of category membership given some set of features). It’s also not available in the &lt;strong&gt;R&lt;/strong&gt; statistics library. You can sample from it using MCMC!&lt;/p&gt;
&lt;p&gt;The Dirichlet distribution takes &lt;span class=&#34;math inline&#34;&gt;\(K \geq 2\)&lt;/span&gt; parameters (corresponding to separate dimensions) which are commonly represented as a vector &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol \alpha\)&lt;/span&gt;. Each &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{i} \geq 0\)&lt;/span&gt; is a concentration parameter which specifies how much of the probability mass of the function is concentrated on that dimension.&lt;/p&gt;
&lt;p&gt;For a vector &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol x\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Dirichlet}(\boldsymbol x) = \frac{1}{B\left(\boldsymbol\alpha\right)} \prod_{i=1}^{K}{x_{i}^{\alpha_{i} - 1}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is a normalizing constants: The &lt;span class=&#34;math inline&#34;&gt;\(Beta\)&lt;/span&gt; function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Beta}(\boldsymbol \alpha) = \frac{\prod_{i=1}^{K}{\Gamma\left(\alpha_{i}\right)}}{\Gamma\left(\sum_{i=1}^{K}\alpha_{i}\right)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And &lt;span class=&#34;math inline&#34;&gt;\(\Gamma\)&lt;/span&gt; is the continuous generalization of the factorial (&lt;span class=&#34;math inline&#34;&gt;\(x!\)&lt;/span&gt;) function. You can call it in &lt;strong&gt;R&lt;/strong&gt; with &lt;code&gt;gamma(x)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can use Metropolis to sample from the Dirichlet distribution in the same way you did for the Exponential distribution with one wrinkle: You need a different proposal distribution. The exponential distribution is defined over all positive real numbers, so you used Gaussian proposals and simply cut the distribution off when &lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; 0\)&lt;/span&gt;. However, the Dirichlet distribution is defined only when the value on every dimension is between 0 and 1, and the sum of the values across the dimensions adds up to 1 (this is called a simplex). So if you try to make Gaussian proposals, almost every proposal will be rejected because the Dirichlet distribution will give probability 0 for it.&lt;/p&gt;
&lt;p&gt;Instead, you can use a proposal function where you generate values from a uniform distribution between a small negative and small positive fraction, and this fraction to one dimension while subtracting it from the other. This will keep your &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;s in the desired range.&lt;/p&gt;
&lt;div id=&#34;problem-4-implement-the-beta-function-the-dirichlet-distribution-probability-function-and-the-delta-proposal-function.-the-equations-will-be-different-from-the-functions-above-but-the-process-should-be-very-similar-2-points.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 4&lt;/strong&gt;: Implement the Beta function, the Dirichlet distribution probability function, and the Delta proposal function. The equations will be different from the functions above, but the process should be very similar (2 points).&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5-implement-a-metropolis-sampler-for-a-dirichlet-distribution-and-use-it-draw-samples-from-a-dirichlet-distribution-with-alpha-.74.-how-many-burn-in-samples-do-you-need-before-the-chain-stabilizes-2-points&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 5&lt;/strong&gt;: Implement a Metropolis sampler for a Dirichlet distribution and use it draw samples from a Dirichlet distribution with &lt;span class=&#34;math inline&#34;&gt;\(\alpha = &amp;lt;.7,4&amp;gt;\)&lt;/span&gt;. How many burn in samples do you need before the chain stabilizes? (2 points)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;inference-via-markov-chain-monte-carlo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inference via Markov Chain Monte Carlo&lt;/h2&gt;
&lt;p&gt;In addition to it’s utility in sampling from complex distributions, MCMC is a powerful tool for inference in these same distributions. If we have samples from a distribution with unknown parameters, we can use MCMC to infer the parameters that generated it (e.g. we can infer the category structure most likely to have produced the exemplars we observed).&lt;/p&gt;
&lt;p&gt;In the final part of the assignment, you will use Metropolis to infer the unknown parameters of a Dirichlet distribution from a set of samples from it. To do this, we will leverage Bayes’ rule: &lt;span class=&#34;math inline&#34;&gt;\(P\left(H|D\right) \propto P\left(D|H\right)P(H)\)&lt;/span&gt;. In the previous section, you used the likelihood function for a known Dirichlet distribution &lt;span class=&#34;math inline&#34;&gt;\(P(D|H)\)&lt;/span&gt; in order to draw samples from it. Now you will use that same likelihood function for inference. The intuition here is that the parameters that assign high probability to a given sample are more likely to have been the source of that sample.&lt;/p&gt;
&lt;p&gt;In the previous sections, in each iteration you proposed a sample from the distribution and rejected it if it was not sufficiently likely. Now, you will propose hypothetical parameters for the distribution, and reject them if they do not make the sample sufficiently likely.&lt;/p&gt;
&lt;p&gt;The homework template will provide you 100 samples from this unknown Dirichlet distribution and your job will be to recover the parameters that generated it. Think carefully about the proposal distribution you want to use here. Remember, each proposal will now move you around on the parameter space of the Dirichlet distribution–not the space of values produced by the Dirichlet.&lt;/p&gt;
&lt;div id=&#34;problem-6-implement-the-compound-dirichlet-probability-function-and-an-appropriate-proposal-function.-this-compound-probability-distribution-should-take-in-a-list-of-samples-and-a-set-of-parameters-alpha-and-return-the-probability-of-generating-all-of-the-samples-from-a-dirichlet-distribution-with-the-given-parameters.-2-points.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 6&lt;/strong&gt;: Implement the compound Dirichlet probability function and an appropriate proposal function. This compound probability distribution should take in a list of samples and a set of parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, and return the probability of generating all of the samples from a Dirichlet distribution with the given parameters. (2 points).&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-7-run-your-new-mcmc-sampler-to-determine-the-generating-parameters.-you-may-need-to-play-around-a-bit-with-proposal-distributions-and-number-of-samplesburn-in-2-points.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Problem 7&lt;/strong&gt;: Run your new MCMC sampler to determine the generating parameters. You may need to play around a bit with proposal distributions and number of samples/burn in (2 points).&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
