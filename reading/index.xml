<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reading policy | Learning in Humans and Machines</title>
    <link>/learning-humans-machines/reading/</link>
      <atom:link href="/learning-humans-machines/reading/index.xml" rel="self" type="application/rss+xml" />
    <description>Reading policy</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <image>
      <url>/learning-humans-machines/img/social-image.png</url>
      <title>Reading policy</title>
      <link>/learning-humans-machines/reading/</link>
    </image>
    
    <item>
      <title>Bayesian associative learning</title>
      <link>/learning-humans-machines/reading/15-reading/</link>
      <pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/reading/15-reading/</guid>
      <description>


&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Gershman, S., J. (2015). &lt;a href=&#34;gershman2015.pdf&#34;&gt;A unifying probabilistic view of associative learning&lt;/a&gt;. &lt;em&gt;PLoS Computational Biology&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;, e1004567.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should read the first ~half of the paper until the start of the temporal difference section (bottom of page 7). There’s a great &lt;a href=&#34;https://www.youtube.com/watch?v=K5RVbXeDE5A&#34;&gt;youtube talk&lt;/a&gt;-form of this paper from Sam Gershman that is really helpful for understanding the paper. The part for the first half of the paper ends at ~35 minutes. You can watch this instead of or in addition to the reading if you like.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Your goal should be to understand why this model is different from the Rescorla-Wagner model that you learned about the start of the semester, and be able to talk about their relative merits, and how they relate to higher-level ideas about frameworks for thinking about learning.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Gershman, S., J., &amp;amp; Niv, Y. (2012). &lt;a href=&#34;gershman2012.pdf&#34;&gt;Exploring a latent cause theory of classical conditioning&lt;/a&gt;. &lt;em&gt;Learning &amp;amp; Behavior&lt;/em&gt;, &lt;em&gt;40&lt;/em&gt;, 255—268.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You should understand what motivated this model (relative to classical conditioning models), the phenomena described, and why the model accounts for them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The primary goal this week is to talk about how this framing of the learning problem is different from prior models of classical conditioning, what it buys us, and how compelling you find this account.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Models at different levels</title>
      <link>/learning-humans-machines/reading/12-reading/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/reading/12-reading/</guid>
      <description>


&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Colunga, E., &amp;amp; Smith, L. B. (2005). &lt;a href=&#34;colunga2005.pdf&#34;&gt;From the lexicon to expectations about kinds: a role for associative learning&lt;/a&gt;. &lt;em&gt;Psychological Review&lt;/em&gt;, &lt;em&gt;112&lt;/em&gt;, 347—382.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read the introduction, Experiments 1-3, and the discussion and conclusion. Your goal should be to understand what the phenemon being modeled is, how the model works, and what the basic results are.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Kemp, C., Perfors, A., &amp;amp; Tenenbaum, J. B. (2007). &lt;a href=&#34;kemp2007.pdf&#34;&gt;Learning overhypotheses with hierarchical Bayesian models.&lt;/a&gt;. &lt;em&gt;Developmental Science&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;, 307—321.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can skip the section on ontological kinds. Your goal should again be to understand what the model is doing and why it produces the results it does.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The primary goal this week is to think about the relationship between these two models. How are they the same? How are they different? Are there reasons to prefer one to the other? Are there some things that one does better than the other?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Limits to connectionism</title>
      <link>/learning-humans-machines/reading/08-reading/</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/reading/08-reading/</guid>
      <description>


&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Marcus, G. F., Vijayan, S., Rao, S. B., &amp;amp; Vishton, P. M. (1999). &lt;a href=&#34;marcus1999.pdf&#34;&gt;Rule learning by seven-month-old infants&lt;/a&gt;. &lt;em&gt;Science&lt;/em&gt;, &lt;em&gt;283&lt;/em&gt;, 77—80.
&lt;strong&gt;Also read the responses&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your goal here should be to understand Marcus et al.’s experimental paradigm and why they think it means that human cognition cannot operate the way that neural networks do. Do you agree? Do you find the criticisms compelling?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; McClelland, J. L., &amp;amp; Plaut, D. C. (1999). &lt;a href=&#34;mcclelland1999.pdf&#34;&gt;Does generalization in infant learning implicate abstract algebra-like rules?&lt;/a&gt;. &lt;em&gt;Trends in Cognitive Sciences&lt;/em&gt;, &lt;em&gt;3&lt;/em&gt;, 166—168.
&lt;strong&gt;Also read the Marcus response&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is a more detailed objection to the Marcus (1999) argument. Make sure you understand what they are suggesting that networks can learn, and also why Marcus is not impressed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt;Marcus, G. (2018). &lt;a href=&#34;marcus2018.pdf&#34;&gt;Deep learning: A critical appraisal&lt;/a&gt;. &lt;em&gt;arXiv preprint&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Neural networks in 2018 are much more impressive than they were in 1999. And yet, Marcus is still concerned. As you read this, think about whether the same arguments are being made here as in his 1999 paper. Are previous arguments refuted? Are there new compelling arguments?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How far can simple associative learning get you?</title>
      <link>/learning-humans-machines/reading/04-reading/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/reading/04-reading/</guid>
      <description>


&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Ramscar, M., Yarlett, D., Dye, M., Denny, K., &amp;amp; Thorpe, K. &lt;a href=&#34;ramscar2010.pdf&#34;&gt;The Effects of feature-label-order and their implications for symbolic learning&lt;/a&gt;. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;, 909-957.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is a challenging paper—both the theory and the model are pretty dense. If you don’t understand all of the distinctions that the authors are drawing between reference and prediction don’t worry too much about that (section 1). But make sure you understand sections 2-8. The rest of the paper is optional.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Smith, L. B. &lt;a href=&#34;smith2000.pdf&#34;&gt;Learning how to learn words: An associative crane&lt;/a&gt;. In R. Golinkoff, &amp;amp; K. Hirsh-Pasek (Eds.), &lt;em&gt;Breaking the word learning barrier&lt;/em&gt; (pp. 51-80). Oxford: Oxford University Press.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This paper should be a little bit easier. Make sure you understand the primary argument about the difference between cranes and skyhooks as explanations, and how that maps onto theories of word learning according to Smith.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
