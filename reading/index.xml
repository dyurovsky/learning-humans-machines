<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reading policy | Learning in Humans and Machines</title>
    <link>/learning-humans-machines/reading/</link>
      <atom:link href="/learning-humans-machines/reading/index.xml" rel="self" type="application/rss+xml" />
    <description>Reading policy</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <image>
      <url>/learning-humans-machines/img/social-image.png</url>
      <title>Reading policy</title>
      <link>/learning-humans-machines/reading/</link>
    </image>
    
    <item>
      <title>Models at different levels</title>
      <link>/learning-humans-machines/reading/12-reading/</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/reading/12-reading/</guid>
      <description>
&lt;script src=&#34;/learning-humans-machines/learning-humans-machines/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Colunga, E., &amp;amp; Smith, L. B. (2005). &lt;a href=&#34;colunga2005.pdf&#34;&gt;From the lexicon to expectations about kinds: a role for associative learning&lt;/a&gt;. &lt;em&gt;Psychological Review&lt;/em&gt;, &lt;em&gt;112&lt;/em&gt;, 347—382.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read the introduction, Experiments 1-3, and the discussion and conclusion. Your goal should be to understand what the phenemon being modeled is, how the model works, and what the basic results are.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Kemp, C., Perfors, A., &amp;amp; Tenenbaum, J. B. (2007). &lt;a href=&#34;kemp2007.pdf&#34;&gt;Learning overhypotheses with hierarchical Bayesian models.&lt;/a&gt;. &lt;em&gt;Developmental Science&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;, 307—321.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can skip the section on ontological kinds. Your goal should again be to understand what the model is doing and why it produces the results it does.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The primary goal this week is to think about the relationship between these two models. How are they the same? How are they different? Are there reasons to prefer one to the other? Are there some things that one does better than the other?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Limits to connectionism</title>
      <link>/learning-humans-machines/reading/08-reading/</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/reading/08-reading/</guid>
      <description>


&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Marcus, G. F., Vijayan, S., Rao, S. B., &amp;amp; Vishton, P. M. (1999). &lt;a href=&#34;marcus1999.pdf&#34;&gt;Rule learning by seven-month-old infants&lt;/a&gt;. &lt;em&gt;Science&lt;/em&gt;, &lt;em&gt;283&lt;/em&gt;, 77—80.
&lt;strong&gt;Also read the responses&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your goal here should be to understand Marcus et al.’s experimental paradigm and why they think it means that human cognition cannot operate the way that neural networks do. Do you agree? Do you find the criticisms compelling?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; McClelland, J. L., &amp;amp; Plaut, D. C. (1999). &lt;a href=&#34;mcclelland1999.pdf&#34;&gt;Does generalization in infant learning implicate abstract algebra-like rules?&lt;/a&gt;. &lt;em&gt;Trends in Cognitive Sciences&lt;/em&gt;, &lt;em&gt;3&lt;/em&gt;, 166—168.
&lt;strong&gt;Also read the Marcus response&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is a more detailed objection to the Marcus (1999) argument. Make sure you understand what they are suggesting that networks can learn, and also why Marcus is not impressed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt;Marcus, G. (2018). &lt;a href=&#34;marcus2018.pdf&#34;&gt;Deep learning: A critical appraisal&lt;/a&gt;. &lt;em&gt;arXiv preprint&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Neural networks in 2018 are much more impressive than they were in 1999. And yet, Marcus is still concerned. As you read this, think about whether the same arguments are being made here as in his 1999 paper. Are previous arguments refuted? Are there new compelling arguments?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How far can simple associative learning get you?</title>
      <link>/learning-humans-machines/reading/04-reading/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/learning-humans-machines/reading/04-reading/</guid>
      <description>


&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Ramscar, M., Yarlett, D., Dye, M., Denny, K., &amp;amp; Thorpe, K. &lt;a href=&#34;ramscar2010.pdf&#34;&gt;The Effects of feature-label-order and their implications for symbolic learning&lt;/a&gt;. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;, 909-957.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is a challenging paper—both the theory and the model are pretty dense. If you don’t understand all of the distinctions that the authors are drawing between reference and prediction don’t worry too much about that (section 1). But make sure you understand sections 2-8. The rest of the paper is optional.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-file-alt&#34;&gt;&lt;/i&gt; Smith, L. B. &lt;a href=&#34;smith2000.pdf&#34;&gt;Learning how to learn words: An associative crane&lt;/a&gt;. In R. Golinkoff, &amp;amp; K. Hirsh-Pasek (Eds.), &lt;em&gt;Breaking the word learning barrier&lt;/em&gt; (pp. 51-80). Oxford: Oxford University Press.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This paper should be a little bit easier. Make sure you understand the primary argument about the difference between cranes and skyhooks as explanations, and how that maps onto theories of word learning according to Smith.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
