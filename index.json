[{"authors":["dan"],"categories":null,"content":"Dan Yurovsky is an assistant professor at the Department of Psychology at Carnegie Mellon University, researching language acquisition and cognitive development.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"bc34d76bc3e159b0c10dc7a9fb6aef07","permalink":"https://dyurovsky.github.io/learning-humans-machines/authors/dan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/authors/dan/","section":"authors","summary":"Dan Yurovsky is an assistant professor at the Department of Psychology at Carnegie Mellon University, researching language acquisition and cognitive development.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"  I have included a bunch of extra resources and guides related to causal inference, program evaluation, R, data, and other relevant topics. Enjoy!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1603912312,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/resource/","section":"resource","summary":"I have included a bunch of extra resources and guides related to causal inference, program evaluation, R, data, and other relevant topics. Enjoy!","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":" I will post all the materials (slides, example code, in-class activities, etc.) from each class session on the day of the class. I typically don’t post the slides before class, but they will be posted after.\n","date":1605744000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1605744000,"objectID":"108da05078d325a5a1f01a1ff2583053","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/class/","section":"class","summary":"I will post all the materials (slides, example code, in-class activities, etc.) from each class session on the day of the class. I typically don’t post the slides before class, but they will be posted after.","tags":null,"title":"Class details","type":"docs"},{"authors":null,"categories":null,"content":" Completing the readings before each class is an essential part of this course. Your seminar participation is a substantial part of your grade, so please completed readings before each class session.\n","date":1605571200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1605571200,"objectID":"40fcd2da3bf2dc718a2fe044c31cdc56","permalink":"https://dyurovsky.github.io/learning-humans-machines/reading/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/reading/","section":"reading","summary":"Completing the readings before each class is an essential part of this course. Your seminar participation is a substantial part of your grade, so please completed readings before each class session.","tags":null,"title":"Reading policy","type":"docs"},{"authors":null,"categories":null,"content":"  Homeworks Final project   Homeworks To practice thinking about, evaluating, and implementing models, you will complete a series of homework assignments roughly every-other week. You may (and should!) work together on the homeworks, but you must turn in your own answers.\n Final project At the end of the course, you will demonstrate your knowledge There is no final exam. This project is your final exam.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1603912312,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"https://dyurovsky.github.io/learning-humans-machines/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/assignment/","section":"assignment","summary":"  Homeworks Final project   Homeworks To practice thinking about, evaluating, and implementing models, you will complete a series of homework assignments roughly every-other week. You may (and should!) work together on the homeworks, but you must turn in your own answers.\n Final project At the end of the course, you will demonstrate your knowledge There is no final exam. This project is your final exam.\n ","tags":null,"title":"Assignment details","type":"docs"},{"authors":null,"categories":null,"content":"   Evidence, causation, and evaluation Regression and inference Theories of change and measurement Counterfactuals and DAGs Threats to validity   Evidence, causation, and evaluation You should understand…\n …the difference between experimental research and observational research …the sometimes conflicting roles of science and intuition in public administration and policy …the difference between the various types of evaluations and how they target specific parts of a logic model …the difference between identifying correlation (math) and identifying causation (philosophy and theory) …what it means for a relationship to be causal   Regression and inference You should understand…\n …the difference between correlation coefficients and regression coefficients …the difference between outcome/response/dependent and explanatory/predictor/independent variables …the two purposes of regression …what each of the components in a regression equation stand for, in both “flavors” of notation:  \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\) for the statistical flavor \\(y = \\alpha + \\beta x_1 + \\gamma x_2 + \\epsilon\\) for the econometrics flavor  …how sliders and switches work as metaphors for regression coefficients …what it means to hold variables constant (or to control for variables) …the different elements of the grammar of graphics and be able to identify how variables are encoded in a graph (i.e. how columns in a dataset can be represented through x/y coordinates, through color, through size, through fill, etc.)  You should be able to…\n …write and interpret R code that calculates summary statistics for groups (i.e. group_by() %\u0026gt;% summarize()) …write and interpret R code that builds linear models (i.e. lm()) …interpret regression coefficients …interpret other regression diagnostics like \\(R^2\\) …use the %\u0026gt;% pipe in R to chain functions together …use ggplot() to visualize data  Helpful resources:\n  Garrett Grolemund and Hadley Wickham, R for Data Science  Kieran Healy, “How ggplot works,” chapter 3 in Data Visualization: A Practical Introduction   Theories of change and measurement You should understand…\n …how to describe a program’s theory of change …the difference between inputs, activities, outputs, and outcomes …the elements of a program’s impact theory: causes (activities) linked to effects (outcomes) …the elements of a program’s logic model: the explicit links between all its inputs, activities, outputs, and outcomes …the difference between implicit and articulated program theories …the purpose of smaller-scale mechanism testing …how indicators can be measured at different levels of abstraction …what makes an indicator a good indicator  You should be able to…\n …identify a program’s underlying theory based on its mission statement …draw a program impact theory chart that links activities to outcomes …draw a program logic model that links inputs to activities to outputs to outcomes …identify the most central elements of a potential outcome measurement   Counterfactuals and DAGs You should understand…\n …how a causal model encodes our understanding of a causal process …how to identify front door and back door paths between treatment/exposure and outcome …why we avoid closing front door paths …why we close back door paths …why adjusting for colliders can distort causal effects …the difference between logic models and DAGs …the difference between individual level causal effects, average treatment effects (ATE), conditional average treatment effect (CATE), average treatment on the treated effects (ATT), and average treatment on the untreated (ATU) …what the fundamental problem of causal inference is and how we can attempt to address it  You should be able to…\n …draw a possible DAG for a given causal relationship …identify all pathways between treatment/exposure and outcome …identify which nodes in the DAG need to be adjusted for (or closed) …identify colliders (which should not be adjusted for)  Helpful resources:\n  Malcom Barrett, “An Introduction to Directed Acyclic Graphs”  Malcom Barrett, “An Introduction to ggdag”  Judea Pearl, “A Crash Course in Good and Bad Control”: A quick summary of back doors, front doors, confounders, colliders, and when to control/not control for DAG nodes  Causal Inference Bootcamp, “Average Treatment Effects,” Duke University  Causal Inference Bootcamp, “Unit Level Effects,” Duke University  Causal Inference Bootcamp, “Conditional Average Treatment Effects,” Duke University  Causal Inference Bootcamp, “Counterfactuals,” Duke University  Neel Ocean, “Understanding Selection Bias”: explanation of how to identify selection bias from the ATT and the ATE, with an explanation of how ATE = ATT + selection bias under the potential outcomes framework  Paul Hünermund, “Sample Selection vs. Selection Into Treatment”   Threats to validity You should understand…\n …what it means when a study has internal validity and know how to identify the major threats to internal validity, including: omitted variable bias (selection and attrition), trend issues (maturation, secular trends, seasonality, testing, regression to the mean), study calibration issues (measurement error, time frame of study), and contamination issues (Hawthorne effects, John Henry effects, spillovers, and intervening events) …why selection bias is the most pernicious and difficult threat to internal validity and how we can account for it …what it means when a study has external validity …what it means when the measures used in a study have construct validity …what it means when the analysis used in a study has statistical conclusion validity  You should be able to…\n …identify existing and potential threats to validity in a study …suggest ways of addressing these threats  Helpful resources:\n Really, just google “threats to internal validity” or “threats to external validity” and you’ll find a billion different slide decks, articles, and lessons about these. They’re a pretty standard part of any research design class.   ","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"321645e7d4ea6c0724c6a04f77432675","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/exam1/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/learning-humans-machines/resource/exam1/","section":"resource","summary":"Evidence, causation, and evaluation Regression and inference Theories of change and measurement Counterfactuals and DAGs Threats to validity   Evidence, causation, and evaluation You should understand…\n …the difference between experimental research and observational research …the sometimes conflicting roles of science and intuition in public administration and policy …the difference between the various types of evaluations and how they target specific parts of a logic model …the difference between identifying correlation (math) and identifying causation (philosophy and theory) …what it means for a relationship to be causal   Regression and inference You should understand…","tags":null,"title":"Things you should know for Exam 1","type":"docs"},{"authors":null,"categories":null,"content":"  The Rescorla-Wagner Model Simple conditioning Extinction Blocking Conditioned inhibition   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw1.Rmd, and please knit the Markdown file in your completed submission.\nThe Rescorla-Wagner Model The Rescorla-Wagner model, developed by Robert Rescorla and Allen Wagner in 1972, was extremely influential at the time of its publication because it was able to explain several puzzling findings in Pavlovian condition, especially the phenomenon of blocking. It has since been extended by researchers working in Reinforcement learning to account for a number of other interesting phenomena. It is also the basis of the delta rule used for training simple neural networks, as you’ll see later on the course.\nYou’ll work with the same simplified version of the model that you saw in class. The model describes the change in strength associated with a conditioned stimulus (\\(\\Delta V\\)) with this equation:\n\\[\\Delta V = \\alpha \\cdot \\left(\\lambda - V_{total}\\right)\\]\nProblem 1: Describe, in words, what each symbol in this equation means and how it is related to learning (1 point). Now let’s turn this equation into R code. You’ll write a function called rw_delta_v that computes \\(\\Delta V\\) according to the Rescorla-Wagner equation. It should take all three of relevant parameters(\\(\\alpha\\), \\(\\lambda\\), and \\(V_{total}\\)). It should return the amount that the target weight will change.\n Problem 2: Write rw_delta_v (1 point). You’ll use the stub in the R Markdown file in the GitHub Repository that looks like this:\nrw_delta_v \u0026lt;- function(Vtotal, alpha = .1, lambda = 1) { } One thing to notice is that function parameters in R can have defaults specified. If you don’t pass in a value for that parameter, it will get the default value inside the function.\n  Simple conditioning To get started, you’ll simulate of simple conditioning experiment in which someone experiences 10 trials of positive reinforcement in response to a conditioned stimulus. You’ll want to produce a plot of the strength of the conditioned stimulus over the course of these 10 trials so you can see the changes that the model predicts.\nThere are lots of ways of setting up this experiment in R, and you’re welcome to do it however you like. In case you’d like a hand to get started, he’s one strategy:\n Make a tibble with 2 columns: trial and V. The trial column will have the values \\(0\\) through \\(10\\), and the V column will start as all \\(0\\)s.\n Write a for loop that iterates over the numbers \\(1\\) through \\(10\\)—these will index into the rows of your tibble. Set the value of the V column in each row to the result of calling your rw_delta_v function on the value of V in the previous row.\n Make a plot with trial on the x-axis and V on the y-axis. You can make one plot for all four of your simulations if you’re feeling comfortable with the tidyverse, or 4 separate plots.\n  Problem 3: Run 4 simulations of the experiment–try 2 different levels of \\(\\alpha\\) and 2 different levels of \\(\\lambda\\). What effect does these parameters have on the model predictions? (3 points).   Extinction Now let’s see what happens if we take the reinforcer away. Set up a simulation where the participant is exposed to 10 trials in of positive reinforcement in response to a conditioned stimulus, and then 10 trials in which they get no reinforcement (\\(\\lambda = 0\\)). Then make a plot of \\(V\\) over the course of the experiment.\nProblem 4: Try the same parameter values that you used above in this new experiment. How does the extinction curve depend on \\(\\alpha\\) and \\(\\lambda\\)? (2 points). This should be a fairly straightforward extension of the code you wrote for the last Problem. The critical thing will be to make sure that you are using the right value of the \\(\\lambda\\) parameter on each trial—remember, no reinforcer should have no reward and thus \\(\\lambda = 0\\).\n  Blocking Now you’re ready to test Rescorla-Wagner’s ability to account for the Blocking phenomenon. In this simulation, you’ll have two cues–x and y. For the first 10 trials, the simulated participant will be exposed to cue x and be positively reinforced. Then, on the subsequent 30 trials, both x and y will be present and the participant will get reinforced.\nOne way to make this simulation work is to replace the V column with two new columns: Vx and Vy. And then include two more columns in your tibble, x_present and y_present, which indicate whether each cue is present on each trial. You then want to make sure that you simulate updating the weight for all cues that are present \\(\\Delta V\\). And make sure that \\(V_{total}\\) has the right value on each trial!\nProblem 5: Implement the blocking experiment described above and make a plot of the weights of each of the two cues over the course of the 40 trials. You can pick any values for \\(\\alpha\\) and \\(\\lambda\\) (3 points).   Conditioned inhibition Finally, you’re ready to simulate a phenomenon we talked about in class but that you didn’t see directly: Conditioned inhibition. The setup is similar to blocking–first one cue (x) is presented and reinforced, and then x and another cue y appear together. But, this time their combination is not reinforced. As a result, people (and the model) learn a negative value for y. Intuitively, y is the reason that x appearing did not lead to a positive reinforcer.\nProblem 6: Implement the a conditioned inhibition experiment just like your blocking experiment above, except this time without reinforcement on the 30 trials with both stimuli. Make a plot of the values of the two cues over the course of the experiment. Pick any values for \\(\\alpha\\) and \\(\\lambda\\) (2 points).   ","date":1599609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599609600,"objectID":"4ba2d603b0ca245a3bd1a69b53c8f8d1","permalink":"https://dyurovsky.github.io/learning-humans-machines/assignment/01-rescorla-wagner/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/learning-humans-machines/assignment/01-rescorla-wagner/","section":"assignment","summary":"The Rescorla-Wagner Model Simple conditioning Extinction Blocking Conditioned inhibition   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw1.Rmd, and please knit the Markdown file in your completed submission.","tags":null,"title":"Homework 1","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Code Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Code McMurray (2007) model – html and Rmd.\n Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Papers McMurray, B. (2007). Defusing the childhood vocabulary explosion. Science, 317, 631.\n ","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"ac47977a15b3902ca3402f61e5bf9df2","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/01-class/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/learning-humans-machines/class/01-class/","section":"class","summary":"Slides Code Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Code McMurray (2007) model – html and Rmd.\n Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today?","tags":null,"title":"Learning in humans and machines","type":"docs"},{"authors":null,"categories":null,"content":"   RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse Install tinytex    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nRStudio.cloud R is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nGo to https://rstudio.cloud/ and create an account. You’ll receive a link to join the shared class workspace separately. If you don’t get this link, let me know and I will invite you.\n RStudio on your computer RStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\nInstall R First you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n Click on “Download R for XXX”, where XXX is either Mac or Windows:\n If you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.\n If you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n  Double click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n If you use macOS, download and install XQuartz. You do not need to do this on Windows.\n   Install RStudio Next, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n Double click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n  Double click on RStudio to run it (check your applications folder or start menu).\n Install tidyverse R packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n Install tinytex When you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced “lay-tek” or “lah-tex”; for goofy nerdy reasons, the x is technically the “ch” sound in “Bach”, but most people just say it as “k”—saying “layteks” is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB! To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console. Run tinytex::install_tinytex() in the console. Wait for a bit while R downloads and installs everything you need. The end! You should now be able to knit to PDF.     It’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n   ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/install/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/learning-humans-machines/resource/install/","section":"resource","summary":"RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse Install tinytex    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"   Randomization Matching and inverse probability weighting Difference-in-difference Regression discontinuity Instrumental variables   Randomization  Understand why randomization is crucial for causal inference and counterfactuals Understand the process for analyzing a randomized controlled trial  Crucial resources:\n Readings, slides, and videos for randomization and matching Guide for RCTs Problem set 3 Task 1 in problem set 8   Matching and inverse probability weighting  Understand the intuition behind matching and inverse probability weighting Understand the process for adjusting for confounders and closing backdoors with both matching and inverse probability weighting  Crucial resources:\n Readings, slides, and videos for randomization and matching Guide for matching and inverse probability weighting Problem set 3 Task 2 in problem set 8   Difference-in-difference  Understand the intuition behind making causal inferences with difference-in-differences Understand the process for analyzing diff-in-diffs  Crucial resources:\n Readings, slides, and videos for diff-in-diff Guide for diff-in-diff Problem set 4 Problem set 5 Task 3 in problem set 8   Regression discontinuity  Understand the intuition behind making causal inferences with regression discontinuity Understand the process for analyzing regression discontinuities, both fuzzy and sharp Understand the difference between ATE and LATE  Crucial resources:\n Readings, slides, and videos for regression discontinuity I (sharp) Readings, slides, and videos for regression discontinuity II (fuzzy) Guide for sharp diff-in-diff Guide for fuzzy diff-in-diff Problem set 6 Task 4 in problem set 8   Instrumental variables  Understand the intuition behind using instruments for causal inference Understand the three characteristics of a good instrument Understand the process for analyzing data with instrumental variables and 2SLS Understand the difference between ATE and LATE  Crucial resources:\n Readings, slides, and videos for instrumental variables I Readings, slides, and videos for instrumental variables II Guide for instrumental variables Problem set 7 Task 5 in problem set 8   ","date":1605139200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605245387,"objectID":"fd91e0322b0d6848d873e526d5d08dc1","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/exam2/","publishdate":"2020-11-12T00:00:00Z","relpermalink":"/learning-humans-machines/resource/exam2/","section":"resource","summary":"Randomization Matching and inverse probability weighting Difference-in-difference Regression discontinuity Instrumental variables   Randomization  Understand why randomization is crucial for causal inference and counterfactuals Understand the process for analyzing a randomized controlled trial  Crucial resources:\n Readings, slides, and videos for randomization and matching Guide for RCTs Problem set 3 Task 1 in problem set 8   Matching and inverse probability weighting  Understand the intuition behind matching and inverse probability weighting Understand the process for adjusting for confounders and closing backdoors with both matching and inverse probability weighting  Crucial resources:","tags":null,"title":"Things you should know for Exam 2","type":"docs"},{"authors":null,"categories":null,"content":"  Introduction Perceptrons Multi-layer networks Using the neuralnet package   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw2.Rmd, and please knit the Markdown file in your completed submission.\nIntroduction Neural networks were a revolution in the scientific study of cognitive neuroscience, spawning a large body of work investigating the computational properties of systems designed to model the operation of the brain’s basic computational units (Rosenblatt, 1958. These perceptrons varied along a number of dimensions, but all of them had the same critical flaw: they could not learn non-linear combinations of inputs, leading to their failure on even simple problems like Exclusive Or (XOR; Minsky \u0026amp; Papert, 1967).\nIn the 1980s, work by David Rumelhart and his colleagues rekindled the field’s interest in neural networks by devising an algorithm by which these models could learn non-linear combinations of input and develop genuinely interesting and surprisingly representations of their input (Rumelhart, Hinton, \u0026amp; Williams, 1986). This lead to an explosion of work in artificial neural networks in the following decades, and also, following Marr’s perspective, a reconsideration of what these models were intended to describe (e.g. the neurons in the networks need not map on to neurons in the brain).\nIn this assignment you will first implement simple one-layer perceptrons that learn with the perceptron learning rule. You’ll show that these can learn several logical functions: AND, OR, and NOT. But they cannot learn XOR.\nYou will then build a very simple multi-layer network that uses backpropagation to learn XOR. You’ll finally use the neuralnet package to solve this same problem, and then use it to build a digit classifier using a small version of the MNIST dataset\n Perceptrons Your first goal will be to train a perceptron to solve logical AND. I’ve provided a set of stub functions that scaffold one way of doing this. The idea is to approximate a sort of pseudo-object oriented structure using a named list. This is overkill for just this simple perceptron, but you’ll find that it extends easily to a backprop network.\nThis object defined in the perceptron function. A perceptron is a list that has 5 elements:\nA tibble of inputs\n A list of target ys for those inputs\n A list of outputs for the last run of the network corresponding to these inputs\n A list of activations that occur after applying the sigmoid function to those outputs\n A list of weights – 3 in total. Two that connect from the input nodes (x1, x2), and one that connects from a bias node (1) to the output node (y).\n  This list will track the state of the perceptron as it goes through the training function you write (train_perceptron). You can write this function as two nested for loop, the outer one over iterations, and the inner one over examples. In each run of the inner loop, you will\nRun perceptron_feedforward over the training example.\n Run perceptron_feedback over that training example to update the weights.\n  In each run of the outer loop, you will run perceptron_feedforward and perceptron_feewdback on all of the examples, and then compute the error for that iteration and store it in the network, e.g. perceptron$errors[iteration] \u0026lt;- sq_error(...).You’ll need to make sure you define perceptron$errors as a list of the right length at the start of the train_perceptron function.\nIf you want to use these stubs, your plan of attack should be:\nWrite the helper functions sigmoid and sq_error\n Make the and_data tibble with columns x1, x2, and y that corresponds to the four possible input and output combinations for AND\n Write the perceptron_feedforward and perceptron_feedback functions\n Write train_perceptron\n Run train_perceptron on your AND data, and plot or otherwise summarize the change in errors over time and the final weights.\n  Problem 1: Fill out the stub helper functions sigmoid and sq_error (1 point)  Problem 2: Write code to train a perceptron to solve logical AND. You can use the stubs provided in the hw2.Rmd or write your own. Train the perceptron, and report back on the results. Did the error in the network go down over the course of training? What were the final weights? (3 points)  Problem 3: Use your functions to train the perceptron on OR, NOT X1, and XOR. Which problems did it succeed on. Which did it fail on? What were the final weights for each? Do they make sense? (2 points)   Multi-layer networks If everything went right, you will have discovered in Problem 3 that perceptrons cannot learn to solve non-linear classification problems like XOR. But with a hidden layer, we can fix this problem. You can use the same strategy you used for your perceptron to implement a 2-layer backpropagation network. You might find the backprop by hand example we did helpful for reference for the weight update formulas.\nProblem 4: Write code to train a 2-layer network to learn XOR. You should have 3 hidden layer nodes — 2 that take input from the input layer, and 1 bias node. After training, investigate the hidden layer nodes. What has the network learned? (3 points)   Using the neuralnet package The network you implemented is likely to be pretty inefficient. In practice, most implementations of neural networks use matrix multiplication to compute weights and outputs, which drastically speeds things up. For the last two problems, you’ll be using the neuralnet r package to investigate what these networks can learn in a more interesting problem.\nThe workhorse of the package is the neuralnet function which trains neural nets to solve problems. It uses formula notation just like lm and other standard statistical methods in R.\nProblem 5: Use the provided code to run neuralnet on your xor data, and the plot your neuralnet to see what the weights on each node are. Did it learn the same thing as the network you made from scratch? Now that you understand how this package works, you’ll use it to solve a more interesting classification problem. You’ll be learning to classify handwritten digits from the MNIST dataset that are a classic success story for modern neural networks. You’ll be working with just a small, scaled down subset of the real dataset — 100 examples of each of the digits 0-9. Here is the first example of each:\nYour goal will be to try out the neuralnet package to find what kinds of network structures matter for learning to classify these digits. The representation you’ll work with is a linearized version of the digits — imagine taking all of the rows of these figures and chaining them all into one long row. This means your network won’t have any ability to use the spatial structure of the images. But even with just the pixel values, you’ll find that you can do a fair bit better than chance (\\(\\frac{1}{10}\\)).\nThe neuralnet package uses what is called a one-hot encoding: The output layer will have ten nodes, each corresponding to a digit. The goal of the network when it reads say a \\(9\\) is for all of the output nodes except for the one corresponding to \\(9\\) to have no activation, and for the \\(9\\) node to have 1.\n Problem 6: Play around with the hidden layer structure of the network. You can specify how many units there are. For instance, using the argument hidden = 2 makes 2 hidden layer nodes (plus a bias). using the argument hidden = c(3,2) makes 2 hidden layers - the first having 3 units and the second having 2 (plus a bias). If the number of units gets too big you’ll find that the network crashes or fails to converge. But report back on at least 3 argument values that successfully ran. You can use the provided prediction_error function will compute how much error there is. Make sure you look at error both on the training set (mnist_train) and on a set of examples that it hasn’t been trained on (mnist_test). Is the network overfitting? What is it learning? Feel free to extend the prediction_error function to compute other metrics if you think they would be interesting.\n  ","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"a62ebcb9b73035c44fe4e9ec162a9a74","permalink":"https://dyurovsky.github.io/learning-humans-machines/assignment/02-networks/","publishdate":"2020-09-29T00:00:00Z","relpermalink":"/learning-humans-machines/assignment/02-networks/","section":"assignment","summary":"Introduction Perceptrons Multi-layer networks Using the neuralnet package   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw2.Rmd, and please knit the Markdown file in your completed submission.","tags":null,"title":"Homework 2","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1599091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599091200,"objectID":"9ae245330f59c57e6f62568d635c26e4","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/02-class/","publishdate":"2020-09-03T00:00:00Z","relpermalink":"/learning-humans-machines/class/02-class/","section":"class","summary":"Slides Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"R, RStudio, and Github","type":"docs"},{"authors":null,"categories":null,"content":"  Introduction Modeling the number game The candidate hypotheses Modeling Inference Inferring hypotheses Predicting the next number.   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw3.Rmd, and please knit the Markdown file in your completed submission.\n Introduction An important problem in inferential learning is that the data available are almost always too sparse to unambiguously pick out the true hypothesis (see e.g. Quine, 1960). Human learners, however, appear to cope with this problem just fine, converging on a small set of possible hypothesis from very little data. In machine learning, this is known as few-shot learning, and is a real challenge for modern learning systems. One hypothesis for what helps human learners is that have a strong prior on the kinds of hypotheses that are likely and relevant, which makes the search problem much easier.\nIn this assignment, you’ll be working with a simple problem designed to demonstrate this phenomenon, and building a model that learns in a human-like way. The number game, developed by Josh Tenenbaum (2000), demonstrates the power of strong sampling for inductive inference. The key insight is that the probability that data is generated by a hypothesized process is proportional to the size of the set of possible data that the hypothesis could generate.\nIn the number game, you get as input one or more numbers between 1 and 100 generated by a computer program according to an unknown rule. Your job is to determine the most likely rule, or to make predictions about which numbers the program will generate next.\n Modeling the number game To figure out what hypothesis the computer is generating samples from, you can use Bayesian inference. To do this, you will use Bayes Rule to infer the posterior probability for each hypothesis:\n\\[ P\\left(H \\vert D\\right) \\propto P\\left(D \\vert H\\right)P\\left(H\\right)\\]\nTo specify the model, you need to specify a prior probability (\\(P\\left(H\\right)\\)) for each hypothesis, and a likelihood function that specified how probable the observed data are under that hypothesis (\\(P\\left(D \\vert H\\right)\\)).\nPrior: Following Tenenbaum (2000), we’ll say that hypotheses can be of two kinds: Mathematical and Interval. We’ll say that aprior, there is a \\(\\lambda\\) probability that the true hypothesis is in the set of Mathemtical hypotheses, and a (1-\\(\\lambda\\)) probability that the true hypothesis is in the set of Interval Hypotheses\nLikelihood: If we assume that the every number consistent with a particular hypothesis is equally likely to be generated by the machine, then if every number in \\(D\\) is in \\(H\\), the likelihood that set of numbers \\(D\\) is generated by the hypothesis \\(H\\) is the probability of choosing \\(\\lvert D \\rvert\\) numbers at random from the set of \\(\\lvert H \\rvert\\) numbers (\\(\\frac{1}{\\lvert H \\rvert \\choose \\lvert D \\rvert}\\)). You can compute this in R using the choose function. Otherwise, \\(H\\) cannot generate \\(D\\) and the likelihood is 0.\n The candidate hypotheses Your first goal is to write all of the candidate hypotheses that your model will consider. You don’t need to exhaustively include all of the hypotheses in the original Tenenbaum (2000) model, just the following\nMathematical hypotheses:\n even numbers\n odd numbers\n square numbers\n cube numbers\n prime numbers\n multiples of any number \\(n\\), \\(\\left(3 \\leq n \\leq 12\\right)\\)\n powers of any number \\(n\\), \\(\\left(2 \\leq n \\leq 10\\right)\\)\n  Interval hypotheses:\n decades (\\(\\left\\{1-10,\\; 10-20, \\;\\ldots \\right\\}\\))\n any range \\(1 \\leq n \\leq 100, n \\leq m \\leq 100, \\left\\{n-m\\right\\}\\)\n  One nice way of representing these hypotheses as tibbles with two columns: name which gives a human-readable description of the hypothesis, and set which is a list of all of the numbers 1—100 consistent with that hypothesis. You’ll first. write a function that generates each of these. For the multiple-set hypotheses (e.g. multiples of any number), you can make the function generate a multi-row tibble where each row is one of the sub-hypotheses.\nProblem 1: Define all of the hypotheses for the number game. You’re welcome to use the stubs I have provided in the template. (3 points). Now that you have each of the individual hypotheses, you can put them all together. A nice representation for this is one big tibble that has a row for all of the hypotheses you’ve generated. You’ll find the bind_rows function to be helpful here.\nThe last detail is that you want to be able to assign a prior probability appropriately to each of these hypotheses. Remember, all of the mathematical hypothesis together should have prior probability \\(\\lambda\\), and the interval hypotheses should make up the remainder of the prior (\\(1 - \\lambda\\)). You’ll want to divy up this probability equally within those respective sets.\n Problem 2: Write the function make_all_hypotheses which takes in a value for the parameter \\(\\lambda\\) and construct the full tibble of hypotheses. It should have four columns: name, set, type (mathematical or interval), and prior which have a value of each of all of the possible hypotheses. Hint: There should be 5,084 total (1 point).   Modeling Inference Now that you have all of your hypotheses, you need to write the likelihood function. And finally to compute the posterior probability of each hypothesis.\nProblem 3: Write the function compute_likelihood which takes in a set of numbers consistent with a hypothesis \\(H\\) , and a set of input numbers \\(D\\), and which returns the likelihood of generating that input set from that hypothesized number set (\\(P\\left(D \\vert H\\right)\\)) (1 point).  Problem 4: Write the function best_hypotheses which takes in a tibble of hypotheses (\\(H\\)), a set of input numbers \\(D\\), an optional number of hypotheses \\(n\\), and returns a tibble with the top \\(n\\) hypotheses for the data according to the posterior probability (\\(P\\left(H \\vert D\\right)\\)) as well as their corresponding posterior probability. You might find it helpful not to return every row of the original hypotheses tibble, but only their type (mathematical or interval), name, and posterior (2 points).   Inferring hypotheses Now you are ready to test the model! Using the default \\(\\lambda\\) value of \\(\\frac{4}{5}\\), you will get the top 5 best hypotheses for each of the following sets of data \\(D\\):\n \\(2\\)\n \\(2, 4, 8\\)\n \\(15, 22\\)\n \\(1, 2, 3, 4, 6\\)\n  Problem 5: Print out the top 5 best hypotheses according to the model for each of these values. Does the model predict what you think it should? Were there any surprises? (2 points)  Problem 6: Try changing the value of \\(\\lambda\\) to something smaller, like \\(\\frac{1}{5}\\). What effect does this have on the model’s predictions for the examples above? (1 point)   Predicting the next number. Now you will implement the final part of the model: Prediction about whether the computer is likely to generate a new number \\(d\u0026#39;\\) having observed all of the previous examples \\(D\\). To do this, you will use Bayesian model averaging. Instead of committing to the best hypothesis after observing \\(D\\), you will average together all of the hypotheses, weighing them by the posterior probability of them being true after observing \\(D\\).\n\\[ P\\left(d\u0026#39;\\right) = \\sum_{h \\in H}P\\left(d\u0026#39; | h\\right)P\\left(h | D\\right)\\]\nYou’ll just need to write one new function predict_value which will take in a tibble of hypotheses, a set of previously observed numbers, and a new target number. It should return the posterior probability of observing that number according to the formula above. You shouldn’t need any new helper functions here!\nTo check whether it works correctly, try these examples using the default value for \\(\\lambda\\):\n \\(D = 2, \\;d\u0026#39; = 12\\)\n \\(D = 2, 4, 8, \\;d\u0026#39; = 12\\)\n \\(D = 15, 22 \\; d\u0026#39; = 21\\)\n \\(D = 15, 22 \\; d\u0026#39; = 50\\)\n  Problem 7: Write the predict_value function and test it on the examples above. Did you get the output you expected? (2 points)   ","date":1602633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602633600,"objectID":"bad2d3721a45185b01e6a26bce1506df","permalink":"https://dyurovsky.github.io/learning-humans-machines/assignment/03-bayes/","publishdate":"2020-10-14T00:00:00Z","relpermalink":"/learning-humans-machines/assignment/03-bayes/","section":"assignment","summary":"Introduction Modeling the number game The candidate hypotheses Modeling Inference Inferring hypotheses Predicting the next number.   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw3.","tags":null,"title":"Homework 3","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Papers Rescorla R. A., \u0026amp; Wagner A. R. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In A. H. Black \u0026amp; W. F. Prokasy (Eds.), Classical Conditioning II: Current Research and Theory. New York: Appleton Century Crofts.\n ","date":1599523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599523200,"objectID":"a0c01f3889200201f6df018341b3db8f","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/03-class/","publishdate":"2020-09-08T00:00:00Z","relpermalink":"/learning-humans-machines/class/03-class/","section":"class","summary":"Slides Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Associative learning","type":"docs"},{"authors":null,"categories":null,"content":"   Learning R R in the wild   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n R for Data Science: A free online book for learning the basics of R and the tidyverse. R and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things. Stat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online. STA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. CSE 631: Principles \u0026amp; Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.   R in the wild A popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.1 Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\nHere are some of the best examples I’ve come across:\n Text analysis of Trump’s tweets confirms he writes only the (angrier) Android half (with a follow-up) Bob Ross - Joy of Painting Bechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight. Sexism on the Silver Screen: Exploring film’s gender divide Comparison of Quentin Tarantino Movies by Box Office and the Bechdel Test Who came to vote in Utah’s caucuses? Health care indicators in Utah counties Song lyrics across the United States A decade (ish) of listening to Sigur Rós When is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here. Mapping Fall Foliage General (Attys) Distributions Disproving Approval    If you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That’s actually how this site is built (see the source code). You can build your own site with this tutorial.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"0f6270d48011ac62645a8455a86a24bf","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/resource/r/","section":"resource","summary":"Learning R R in the wild   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.","tags":null,"title":"R","type":"docs"},{"authors":null,"categories":null,"content":"   R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n Main style things to pay attention to for this class  Important note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n Spacing  See the “Spacing” section in the tidyverse style guide.\n Put spaces after commas (like in regular English):\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg , cty \u0026gt; 10) filter(mpg ,cty \u0026gt; 10) filter(mpg,cty \u0026gt; 10) Put spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg, cty\u0026gt;10) filter(mpg, cty\u0026gt; 10) filter(mpg, cty \u0026gt;10) Don’t put spaces around parentheses that are parts of functions:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter (mpg, cty \u0026gt; 10) filter ( mpg, cty \u0026gt; 10) filter( mpg, cty \u0026gt; 10 )  Long lines  See the “Long lines” section in the tidyverse style guide.\n It’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Bad filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;)) # Good filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))  Pipes (%\u0026gt;%) and ggplot layers (+) Put each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad and won\u0026#39;t even work ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() Put each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad and won\u0026#39;t even work mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))  Comments  See the “Comments” section in the tidyverse style guide.\n Comments should start with a comment symbol and a single space: #\n# Good #Bad #Bad If the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group You can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good # Happy families are all alike; every unhappy family is unhappy in its own way. # Everything was in confusion in the Oblonskys’ house. The wife had discovered # that the husband was carrying on an intrigue with a French girl, who had been # a governess in their family, and she had announced to her husband that she # could not go on living in the same house with him. This position of affairs # had now lasted three days, and not only the husband and wife themselves, but # all the members of their family and household, were painfully conscious of it. # Bad # Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Though, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/resource/style/","section":"resource","summary":"R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"  Introduction The Metropolis Algorithm Sampling from an Exponential Distribution Sampling from a Dirichlet Distribution Inference via Markov Chain Monte Carlo   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw4.Rmd, and please knit the Markdown file in your completed submission.\n Introduction In the last decade, we have seen an explosion of impressive results in Machine Learning, producing models that match and even sometimes outperform humans on challenging tasks. A non-trivial part of the challenge in developing these models is not in specifying how they should solve the problems they are developed solve, but in figuring out how to actually derive predictions from these models.\nEarlier this semester, we read a paper from Charles Kemp, And Perfors, and Josh Tenebaum that shows how overhypotheses like the shape bias can be understood through the lens of rational analysis. Because the model does not have an analytic solution, A sizeable chunk of the article describes approximation methods that the authors use for for determining approximately what the model predicts.\nThe approach that Kemp et al. use, and that is a mainstay in modern Bayesian approaches, is technique called Markov chain Monte Carlo (MCMC). Markov chain Monte Carlo is a method for approximating samples from a complex Distribution by drawing samples from other simpler distributions and reweighing or combining them together.\nIn this assignment, you will implement the Metropolis Algorithm, an MCMC algorithm in the sequential sampling family. In this family of algorithms, samples are drawn from a simple Proposal distribution and then either Accepted (if they have high probability under the target distribution), or Rejected in favor of keep the current sample.\n The Metropolis Algorithm Metropolis is a simple sequential sampling scheme for drawing samples from a target distribution \\(P\\left(x\\right)\\) by using a density function \\(f\\left(x\\right)\\) that is proportional to \\(P\\)– i.e., that can give the relative probability of two different samples \\(x\\) and \\(x\u0026#39;\\).\nThe algorithm works by starting a chain at some arbitrary point \\(x_0\\). Then, in each iteration \\(t\\), a new sample \\(x1\\) by modifying the previous sample \\(x_{t}\\) according to a proposal distribution \\(g\\left(x\u0026#39;|x_{t}\\right)\\). The critical requirement is that \\(g\\) is a symmetric distribution \\(g\\left(x\u0026#39;|x\\right) = g\\left(x|x\u0026#39;\\right)\\).\nFor your simulation, you should use a simple Normal distribution centered at the previous sample \\(g\\left(x\u0026#39;|x\\right) \\sim \\text{Normal}\\left(x, \\sigma\\right)\\).\nYou then run the algorithm for some number of iterations, e.g. 1000, at each iteration performing the following steps:\nPropose a new sample \\(x\u0026#39;\\) by drawing from \\(g\\left(x\u0026#39;|x_{t}\\right)\\)\n Compute the acceptance ratio \\(\\alpha = \\frac{f\\left(x\u0026#39;\\right)}{f\\left(x\\right)}\\)\n Sample \\(u \\sim \\text{Uniform}\\left(0,1\\right)\\).\nIf \\(u \\leq \\alpha\\), accept the proposed sample (\\(x_{t+1} = x\u0026#39;\\)).\nOtherwise, reject the proposed sample and keep the previous sample (\\(x_{t+1} = x_{t}\\)).\n   Sampling from an Exponential Distribution Exponential distributions are a common distribution in models of cognition, for example they are sometimes used to model forgetting in memory. The Exponential distribution has one parameter \\(\\lambda\\) which defines the rate of decay (or forgetting).\nIf \\(x \\sim \\text{Exponential}\\left(\\lambda\\right)\\), \\(P\\left(x\\right) = \\lambda e^{-\\lambda x}\\).\nR actually can draw samples from an Exponential distribution. For example, here are 1000 samples from \\(\\text{Exponential}\\left(3\\right)\\)\nsamples \u0026lt;- tibble(sample = 1:1000, value = rexp(1000, rate = 3)) ggplot(samples, aes(x = value)) + geom_histogram(fill = \u0026quot;white\u0026quot;, color = \u0026quot;black\u0026quot;)b But let’s see how can draw samples from an exponential distribution using Metropolis-Hastings samples instead.\nYou will need to do 4 things:\nDefine the Exponential target distribution \\(f(x)\\) using the Exponential definition above.\n Define the proposal distribution \\(g(x\u0026#39;|x)\\)\n Define and Metropolis sequential sampling algorithm\n Run the Algorithm for some number of iterations and plot the results.\n  Problem 1: Implement the Exponential probability density function. This function should take in a value \\(x\\) and a parameter \\(\\lambda\\) and return the probability of sampling the value \\(x\\) from the Exponential distribution with parameter \\(\\lambda\\) (see above, 1 point).  Problem 2: Implement a Gaussian proposal function. This should take in a current value \\(x\\), a parameter sigma which defines how wide the proposal distribution is. It should return a new proposed sample \\(x\u0026#39;\\) (1 point). One thing you might notice is that the Metropolis samples won’t look like Exponential samples right away. Here is a plot of \\(f(x)\\) for 1000 samples from Metropolis:\nggplot(output, aes(x = iteration, y = f)) + geom_line() You can see that first several hundred samples, every sample has low probability under the target distribution. This happens when the starting point of the chain is far away from the area of high density in the target distribution. For this reason, it is common to discard some of the initial samples–a process called burn in. You may find this helpful in future simulations.\n Problem 3: Implement a Metropolis sampler for an Exponential distribution and use it draw 1000 samples from an Exponential distribution with \\(\\lambda=3\\). Try \\(\\lambda=1\\) as well. Do the distributions look different? You should use the two functions you just wrote for Problems 1 and 2. (2 points)   Sampling from a Dirichlet Distribution The Dirichlet Distribution is a highly flexible distribution that is that can many different forms depending on its parameters. It often finds use in Cognitive models precisely when the goal is to learn about something than has unknown shape before learning starts (e.g. the probability of category membership given some set of features). It’s also not available in the R statistics library. You can sample from it using MCMC!\nThe Dirichlet distribution takes \\(K \\geq 2\\) parameters (corresponding to separate dimensions) which are commonly represented as a vector \\(\\boldsymbol \\alpha\\). Each \\(\\alpha_{i} \\geq 0\\) is a concentration parameter which specifies how much of the probability mass of the function is concentrated on that dimension.\nFor a vector \\(\\boldsymbol x\\),\n\\[ \\text{Dirichlet}(\\boldsymbol x) = \\frac{1}{B\\left(\\boldsymbol\\alpha\\right)} \\prod_{i=1}^{K}{x_{i}^{\\alpha_{i} - 1}} \\]\nWhere \\(B\\) is a normalizing constants: The \\(Beta\\) function\n\\[ \\text{Beta}(\\boldsymbol \\alpha) = \\frac{\\prod_{i=1}^{K}{\\Gamma\\left(\\alpha_{i}\\right)}}{\\Gamma\\left(\\sum_{i=1}^{K}\\alpha_{i}\\right)} \\]\nAnd \\(\\Gamma\\) is the continuous generalization of the factorial (\\(x!\\)) function. You can call it in R with gamma(x)\nYou can use Metropolis to sample from the Dirichlet distribution in the same way you did for the Exponential distribution with one wrinkle: You need a different proposal distribution. The exponential distribution is defined over all positive real numbers, so you used Gaussian proposals and simply cut the distribution off when \\(x \u0026lt; 0\\). However, the Dirichlet distribution is defined only when the value on every dimension is between 0 and 1, and the sum of the values across the dimensions adds up to 1 (this is called a simplex). So if you try to make Gaussian proposals, almost every proposal will be rejected because the Dirichlet distribution will give probability 0 for it.\nInstead, you can use a proposal function where you generate values from a uniform distribution between a small negative and small positive fraction, and this fraction to one dimension while subtracting it from the other. This will keep your \\(x\\)s in the desired range.\nProblem 4: Implement the Beta function, the Dirichlet distribution probability function, and the Delta proposal function. The equations will be different from the functions above, but the process should be very similar (2 points).  Problem 5: Implement a Metropolis sampler for a Dirichlet distribution and use it draw samples from a Dirichlet distribution with \\(\\alpha = \u0026lt;.7,4\u0026gt;\\). How many burn in samples do you need before the chain stabilizes? (2 points)   Inference via Markov Chain Monte Carlo In addition to it’s utility in sampling from complex distributions, MCMC is a powerful tool for inference in these same distributions. If we have samples from a distribution with unknown parameters, we can use MCMC to infer the parameters that generated it (e.g. we can infer the category structure most likely to have produced the exemplars we observed).\nIn the final part of the assignment, you will use Metropolis to infer the unknown parameters of a Dirichlet distribution from a set of samples from it. To do this, we will leverage Bayes’ rule: \\(P\\left(H|D\\right) \\propto P\\left(D|H\\right)P(H)\\). In the previous section, you used the likelihood function for a known Dirichlet distribution \\(P(D|H)\\) in order to draw samples from it. Now you will use that same likelihood function for inference. The intuition here is that the parameters that assign high probability to a given sample are more likely to have been the source of that sample.\nIn the previous sections, in each iteration you proposed a sample from the distribution and rejected it if it was not sufficiently likely. Now, you will propose hypothetical parameters for the distribution, and reject them if they do not make the sample sufficiently likely.\nThe homework template will provide you 100 samples from this unknown Dirichlet distribution and your job will be to recover the parameters that generated it. Think carefully about the proposal distribution you want to use here. Remember, each proposal will now move you around on the parameter space of the Dirichlet distribution–not the space of values produced by the Dirichlet.\nProblem 6: Implement the compound Dirichlet probability function and an appropriate proposal function. This compound probability distribution should take in a list of samples and a set of parameters \\(\\alpha\\), and return the probability of generating all of the samples from a Dirichlet distribution with the given parameters. (2 points).  Problem 7: Run your new MCMC sampler to determine the generating parameters. You may need to play around a bit with proposal distributions and number of samples/burn in (2 points).   ","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603670400,"objectID":"9143f06322cce380485ecf954e1a977c","permalink":"https://dyurovsky.github.io/learning-humans-machines/assignment/04-mcmc/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/learning-humans-machines/assignment/04-mcmc/","section":"assignment","summary":"Introduction The Metropolis Algorithm Sampling from an Exponential Distribution Sampling from a Dirichlet Distribution Inference via Markov Chain Monte Carlo   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw4.","tags":null,"title":"Homework 4","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"ac7533426a3114e90edf09c0635b197a","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/04-class/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/learning-humans-machines/class/04-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"How far can simple associative learning get you?","type":"docs"},{"authors":null,"categories":null,"content":"   Unzipping files on macOS Unzipping files on Windows   Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n Unzipping files on Windows tl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n ","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/learning-humans-machines/resource/unzipping/","section":"resource","summary":"Unzipping files on macOS Unzipping files on Windows   Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"  There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.\n Kaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n 360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\n US City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n Political science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n François Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities. Thomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.). Erik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/resource/data/","section":"resource","summary":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Papers Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological review, 65, 386—408.\n ","date":1600128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600128000,"objectID":"0ddf3e2b3a9f5347c88f83033fb01ff2","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/05-class/","publishdate":"2020-09-15T00:00:00Z","relpermalink":"/learning-humans-machines/class/05-class/","section":"class","summary":"Slides Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Perceptrons","type":"docs"},{"authors":null,"categories":null,"content":"  ","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"24a52df01efb615d8b4c5c0f16afc27e","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/regtables/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/learning-humans-machines/resource/regtables/","section":"resource","summary":"  ","tags":null,"title":"Side-by-side regression tables","type":"docs"},{"authors":null,"categories":null,"content":"  You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"68be32a8da6a38dd54a9e724ab3904a0","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/citations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/resource/citations/","section":"resource","summary":"You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.","tags":null,"title":"Citations and bibliography","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things Youtube videos Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Youtube videos 3Blue1BRown - What is backpropogation really doing?\n3Blue1BRown - Backpropogation calculus\n Papers McClelland, J. L., \u0026amp; Rogers, T. T. (2003). The parallel distributed processing approach to semantic cognition. Nature Reviews Neuroscience, 4, 310—322.\nMcClelland, J. L., \u0026amp; Rumelhart, D. E. (1981). An interactive activation model of context effects in letter perception: I. An account of basic findings. Psychological Review, 88, 375—407.\nRumelhart, D. E., Hinton, G. E., \u0026amp; Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323, 533—536.\n ","date":1600300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600300800,"objectID":"119022d4a0cb3ad45bc292e062e9e05a","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/06-class/","publishdate":"2020-09-17T00:00:00Z","relpermalink":"/learning-humans-machines/class/06-class/","section":"class","summary":"Slides Recording Clearest and most confusing things Youtube videos Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Multi-layer networks","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things Backpropopagation example   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Backpropopagation example XOR by hand\n ","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600905600,"objectID":"026700f4d4068083b37f98f89edf4682","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/07-class/","publishdate":"2020-09-24T00:00:00Z","relpermalink":"/learning-humans-machines/class/07-class/","section":"class","summary":"Slides Recording Clearest and most confusing things Backpropopagation example   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Backpropagation details","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600905600,"objectID":"a696c93d764e2bfb447300660fcf2be7","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/08-class/","publishdate":"2020-09-24T00:00:00Z","relpermalink":"/learning-humans-machines/class/08-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"Limits to connectionism","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14, 179—211.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"6fce0ded8aa03d77081bcdf32570ef18","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/09-class/","publishdate":"2020-09-29T00:00:00Z","relpermalink":"/learning-humans-machines/class/09-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14, 179—211.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about?","tags":null,"title":"Recurrent neural networks","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601596800,"objectID":"3fea5612523ac7da2f3ba696739f7198","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/10-class/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/learning-humans-machines/class/10-class/","section":"class","summary":"Slides Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Bayesian inference","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Tenenbaum, J. B. (2000). Rules and similarity in concept learning. In Advances in neural information processing systems (pp. 59—65).\nXu, F., \u0026amp; Tenenbaum, J. B. (2007). Word learning as Bayesian inference. Psychological Review, 114, 245—272..\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"187acf4fd964c835bff8f91f793da537","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/11-class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/class/11-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Tenenbaum, J. B. (2000). Rules and similarity in concept learning. In Advances in neural information processing systems (pp. 59—65).\nXu, F., \u0026amp; Tenenbaum, J. B. (2007). Word learning as Bayesian inference. Psychological Review, 114, 245—272..\n Clearest and most confusing things Go to this form and answer these three questions:","tags":null,"title":"Learning by Bayesian inference","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1602288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602288000,"objectID":"df7a31a1291d87ad849bb7c3fb14d771","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/12-class/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/learning-humans-machines/class/12-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"Models at different levels","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Anderson, J. R., \u0026amp; Schooler, L. J. (1991). Reflections of the environment in memory. Psychological Science, 2, 396—408.\nGriffiths, T. L., \u0026amp; Tenenbaum, J. B. (2006). Optimal predictions in everyday cognition. Psychological Science, 17, 767—773.\nKidd, C., Piantadosi, S. T., \u0026amp; Aslin, R. N. (2012). The Goldilocks effect: Human infants allocate attention to visual sequences that are neither too simple nor too complex. PlOS ONE, 7, e36399.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1602547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602547200,"objectID":"32a6d5319519bb5a196fdac45ee2633c","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/13-class/","publishdate":"2020-10-13T00:00:00Z","relpermalink":"/learning-humans-machines/class/13-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Anderson, J. R., \u0026amp; Schooler, L. J. (1991). Reflections of the environment in memory. Psychological Science, 2, 396—408.\nGriffiths, T. L., \u0026amp; Tenenbaum, J. B. (2006). Optimal predictions in everyday cognition. Psychological Science, 17, 767—773.\nKidd, C., Piantadosi, S. T., \u0026amp; Aslin, R.","tags":null,"title":"Rational Analysis","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Sanborn, A. N., Griffiths, T. L., \u0026amp; Shiffrin, R. M. (2010). Uncovering mental representations with Markov chain Monte Carlo. Cognitive psychology, 60, 63—106.\nVul, E., Goodman, N., Griffiths, T. L., \u0026amp; Tenenbaum, J. B. (2014). One and done? Optimal decisions from very few samples. Cognitive Science, 38, 599—637.\nVul, E., \u0026amp; Pashler, H. (2008). Measuring the crowd within: Probabilistic representations within individuals. Psychological Science, 19, 645—647.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1602720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602720000,"objectID":"a255e2111ec16f362b9c579591551137","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/14-class/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/learning-humans-machines/class/14-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Sanborn, A. N., Griffiths, T. L., \u0026amp; Shiffrin, R. M. (2010). Uncovering mental representations with Markov chain Monte Carlo. Cognitive psychology, 60, 63—106.\nVul, E., Goodman, N., Griffiths, T. L., \u0026amp; Tenenbaum, J. B. (2014). One and done? Optimal decisions from very few samples.","tags":null,"title":"Inference by sampling","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1603152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"b182d133a5ce2faf7a0e7062165a9d0e","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/15-class/","publishdate":"2020-10-20T00:00:00Z","relpermalink":"/learning-humans-machines/class/15-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"Bayesian associative learning","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Code Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Code Pit \u0026amp; Myung (2007) model – html and Rmd.\n Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1603324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603324800,"objectID":"e7d9063a42d1f9a503a7edbecb2c431c","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/16-class/","publishdate":"2020-10-22T00:00:00Z","relpermalink":"/learning-humans-machines/class/16-class/","section":"class","summary":"Slides Code Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Code Pit \u0026amp; Myung (2007) model – html and Rmd.\n Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today?","tags":null,"title":"Comparing models","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1603756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603756800,"objectID":"bee71d806923ae67f51bde86ff6316ad","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/17-class/","publishdate":"2020-10-27T00:00:00Z","relpermalink":"/learning-humans-machines/class/17-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"Machines that learn like people","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Bonawitz, E., Shafto, P., Gweon, H., Goodman, N. D., Spelke, E., \u0026amp; Schulz, L. (2011). The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery. Cognition, 120, 322—330.\nShafto, P., Goodman, N. D., \u0026amp; Frank, M. C. (2012). Learning from others: The consequences of psychological reasoning for human learning. Perspectives on Psychological Science, 7, 341—351.\nShafto, P., Goodman, N. D., \u0026amp; Griffiths, T. L. (2014). A rational account of pedagogical reasoning: Teaching by, and learning from, examples. Cognitive Psychology, 71, 55—89.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1603929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603929600,"objectID":"4dc859b702ac4f3992af0b6d004c4f74","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/18-class/","publishdate":"2020-10-29T00:00:00Z","relpermalink":"/learning-humans-machines/class/18-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Bonawitz, E., Shafto, P., Gweon, H., Goodman, N. D., Spelke, E., \u0026amp; Schulz, L. (2011). The double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery. Cognition, 120, 322—330.\nShafto, P., Goodman, N. D., \u0026amp; Frank, M. C. (2012). Learning from others: The consequences of psychological reasoning for human learning.","tags":null,"title":"Learning from teaching","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1604361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604361600,"objectID":"ff6db1e0082b6af406c28e59aa968f2b","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/19-class/","publishdate":"2020-11-03T00:00:00Z","relpermalink":"/learning-humans-machines/class/19-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"What makes a good teacher?","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Frank, M. C., \u0026amp; Goodman, N. D. (2014). Inferring word meanings by assuming that speakers are informative. Cognitive Psychology, 75, 80—96.\nGoodman, N. D., \u0026amp; Frank, M. C. (2016). Pragmatic language interpretation as probabilistic inference. Trends in Cognitive Sciences, 20, 818—829.\nKao, J. T., Wu, J. Y., Bergen, L., \u0026amp; Goodman, N. D. (2014). Nonliteral understanding of number words. Proceedings of the National Academy of Sciences, 111, 12002—12007.\nYoon, E. J., Tessler, M. H., Goodman, N. D., \u0026amp; Frank, M. C. (in press). Polite speech emerges from competing social goals. Open Mind.\nYurovsky, D., Case, S., \u0026amp; Frank, M. C. (2017). Preschoolers flexibly adapt to linguistic input in a noisy channel. Psychological Science, 28, 132—140.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1604534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604534400,"objectID":"08c65ede8b3049bbeee08624d3f4de7e","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/20-class/","publishdate":"2020-11-05T00:00:00Z","relpermalink":"/learning-humans-machines/class/20-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Frank, M. C., \u0026amp; Goodman, N. D. (2014). Inferring word meanings by assuming that speakers are informative. Cognitive Psychology, 75, 80—96.\nGoodman, N. D., \u0026amp; Frank, M. C. (2016). Pragmatic language interpretation as probabilistic inference. Trends in Cognitive Sciences, 20, 818—829.","tags":null,"title":"Rational speech acts","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1604966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604966400,"objectID":"f337a79c5c3b30619154221e12d6ef0b","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/21-class/","publishdate":"2020-11-10T00:00:00Z","relpermalink":"/learning-humans-machines/class/21-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"Learning indirectly from language","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Lupyan, G., \u0026amp; Dale, R. (2010). Language structure is partly determined by social structure. PLOS ONE, 5, e8559.\nKemp, C., \u0026amp; Regier, T. (2012). Kinship categories across languages reflect general communicative principles. Science, 336, 1049—1054.\nKirby, S., Cornish, H., \u0026amp; Smith, K. (2008). Cumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language. Proceedings of the National Academy of Sciences, 105, 10681—10686.\nKirby, S., Tamariz, M., Cornish, H., \u0026amp; Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87—102.\nSilvey, C., Kirby, S., \u0026amp; Smith, K. (2015). Word meanings evolve to selectively preserve distinctions on salient dimensions. Cognitive Science, 39, 212—226.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1605139200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605139200,"objectID":"43608f28073e2518f9a4be6e19745691","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/22-class/","publishdate":"2020-11-12T00:00:00Z","relpermalink":"/learning-humans-machines/class/22-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Lupyan, G., \u0026amp; Dale, R. (2010). Language structure is partly determined by social structure. PLOS ONE, 5, e8559.\nKemp, C., \u0026amp; Regier, T. (2012). Kinship categories across languages reflect general communicative principles. Science, 336, 1049—1054.\nKirby, S., Cornish, H., \u0026amp; Smith, K.","tags":null,"title":"Iterated learning","type":"docs"},{"authors":null,"categories":null,"content":"   Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"32f17f8bc6e681f78fc26c8a573579e5","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/23-class/","publishdate":"2020-11-17T00:00:00Z","relpermalink":"/learning-humans-machines/class/23-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"Community effects on learning from others","type":"docs"},{"authors":null,"categories":null,"content":"   Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Bedny, M., Koster-Hale, J., Elli, G., Yazzolino, L., \u0026amp; Saxe, R. (2019). There’s more to “sparkle” than meets the eye: Knowledge of vision and light verbs among congenitally blind and sighted individuals. Cognition, 189, 105—115.\nBergey*, C., Morris*, B., \u0026amp; Yurovsky, D. (2020). Children hear more about what is atypical than what is typical. Proceedings of the 41st Annual Conference of the Cognitive Science Society.\nGriffiths, T. L., \u0026amp; Steyvers, M. (2004). Finding scientific topics. Proceedings of the National academy of Sciences, 101, 5228—5235.\nLandauer, T. K., \u0026amp; Dumais, S. T. (1997). A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104, 211—240.\nMarmor, G. S. (1978). Age at onset of blindness and the development of the semantics of color names. Journal of Experimental Child Psychology, 25, 267—278.\nRoy, B. C., Frank, M. C., DeCamp, P., Miller, M., \u0026amp; Roy, D. (2015). Predicting the birth of a spoken word. Proceedings of the National Academy of Sciences, 112, 12663—12668.\nSteyvers, M., \u0026amp; Griffiths, T. (2007). Probabilistic topic models. Handbook of Latent Semantic Analysis, 427, 424—440.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1605744000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605744000,"objectID":"31daa77b95d691a3855a8a5010e8bb30","permalink":"https://dyurovsky.github.io/learning-humans-machines/class/24-class/","publishdate":"2020-11-19T00:00:00Z","relpermalink":"/learning-humans-machines/class/24-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Bedny, M., Koster-Hale, J., Elli, G., Yazzolino, L., \u0026amp; Saxe, R. (2019). There’s more to “sparkle” than meets the eye: Knowledge of vision and light verbs among congenitally blind and sighted individuals. Cognition, 189, 105—115.\nBergey*, C., Morris*, B., \u0026amp; Yurovsky, D.","tags":null,"title":"The structure in language","type":"docs"},{"authors":null,"categories":null,"content":"  Centola, D., \u0026amp; Baronchelli, A. (2015). The spontaneous emergence of conventions: An experimental study of cultural evolution. Proceedings of the National Academy of Sciences, 112, 1989—1994.\n You might also enjoy this talk from the first author on this work here. It’s a little bit long, so feel free to skip around, but it might be helpful for seeing the stimuli and getting a better sense for the experiment.   Hawkins, R.D., Goodman, N.D., Goldberg, A.E. , \u0026amp; Griffiths, T.L. (2020). Generalizing meanings from partners to populations: Hierarchical inference supports convention formation on networks. Proceedings of the 42nd Annual Conference of the Cognitive Science Society.\n You might also enjoy this short talk from the first author on this work.  These papers are both short but pretty tense. Make sure you understand what the authors are trying to demonstrate, how their experiments work, and (roughly) how the models they describe work.\nSign up to lead discussion here: http://bit.ly/lhm-signup\n","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"85533e81a4945604e4ad81bc682b6c54","permalink":"https://dyurovsky.github.io/learning-humans-machines/reading/23-reading/","publishdate":"2020-11-17T00:00:00Z","relpermalink":"/learning-humans-machines/reading/23-reading/","section":"reading","summary":"Centola, D., \u0026amp; Baronchelli, A. (2015). The spontaneous emergence of conventions: An experimental study of cultural evolution. Proceedings of the National Academy of Sciences, 112, 1989—1994.\n You might also enjoy this talk from the first author on this work here. It’s a little bit long, so feel free to skip around, but it might be helpful for seeing the stimuli and getting a better sense for the experiment.","tags":null,"title":"Community effects on learning from others","type":"docs"},{"authors":null,"categories":null,"content":"  Chestnut, E. K., \u0026amp; Markman, E. M. (2018). “Girls are as good as boys at math” implies that boys are probably better: A study of expressions of gender equality. Cognitive Science, 42, 2229—2249.\n Kidd, C., White, K. S., \u0026amp; Aslin, R. N. (2011). Toddlers use speech disfluencies to predict speakers’ referential intentions. Developmental Science, 14, 925—934.\n The last few classes, we have been discussing how speakers structure their language to communicate their intended meaning and how listeners who reason about these intentions can get a lot of learning done. However, speakers can also “leak” unintended information into their language, and listeners can learn from this information too. The two articles for this seminar discuss two such cases. When you read them, think about whether the models we have been working with account for these data, could be extended to account for this data, or whether we need something altogether different to explain them.  Sign up to lead discussion here: http://bit.ly/lhm-signup\n","date":1604966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604966400,"objectID":"707df492ae7ff7997b8fc9469c1ce739","permalink":"https://dyurovsky.github.io/learning-humans-machines/reading/21-reading/","publishdate":"2020-11-10T00:00:00Z","relpermalink":"/learning-humans-machines/reading/21-reading/","section":"reading","summary":"Chestnut, E. K., \u0026amp; Markman, E. M. (2018). “Girls are as good as boys at math” implies that boys are probably better: A study of expressions of gender equality. Cognitive Science, 42, 2229—2249.\n Kidd, C., White, K. S., \u0026amp; Aslin, R. N. (2011). Toddlers use speech disfluencies to predict speakers’ referential intentions. Developmental Science, 14, 925—934.\n The last few classes, we have been discussing how speakers structure their language to communicate their intended meaning and how listeners who reason about these intentions can get a lot of learning done.","tags":null,"title":"Indirectly learning from language","type":"docs"},{"authors":null,"categories":null,"content":"  Gweon, H., Shafto, P., \u0026amp; Schulz, L. (2018). Development of children’s sensitivity to overinformativeness in learning and teaching. Developmental Psychology, 54, 2113—2125.\n Ho, M. K., MacGlashan, J., Littman, M. L., \u0026amp; Cushman, F. (2017). Social is special: A normative framework for teaching with and learning from evaluative feedback. Cognition, 167, 91—106.\n Please read both of these short articles. The first is an extension of the teaching and learning framework we have already discussed. The second is a theoretical argument about the relationship between teaching and reinforcement learning.  Sign up to lead discussion here: http://bit.ly/lhm-signup\n","date":1604361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604361600,"objectID":"867897946ebfb0aeeccb756a89e28c6d","permalink":"https://dyurovsky.github.io/learning-humans-machines/reading/19-reading/","publishdate":"2020-11-03T00:00:00Z","relpermalink":"/learning-humans-machines/reading/19-reading/","section":"reading","summary":"Gweon, H., Shafto, P., \u0026amp; Schulz, L. (2018). Development of children’s sensitivity to overinformativeness in learning and teaching. Developmental Psychology, 54, 2113—2125.\n Ho, M. K., MacGlashan, J., Littman, M. L., \u0026amp; Cushman, F. (2017). Social is special: A normative framework for teaching with and learning from evaluative feedback. Cognition, 167, 91—106.\n Please read both of these short articles. The first is an extension of the teaching and learning framework we have already discussed.","tags":null,"title":"What makes a good teacher?","type":"docs"},{"authors":null,"categories":null,"content":"  Lake, B. M., Ullman, T. D., Tenenbaum, J. B., \u0026amp; Gershman, S. J. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40, e253.\n You should read the entire target article (roughly the first 25 pages). The commentaries and response from the authors are optional.  Sign up to lead discussion here: http://bit.ly/lhm-signup\n","date":1603756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603756800,"objectID":"44a5a6ca24ce8c58c4af215bf9b64340","permalink":"https://dyurovsky.github.io/learning-humans-machines/reading/17-reading/","publishdate":"2020-10-27T00:00:00Z","relpermalink":"/learning-humans-machines/reading/17-reading/","section":"reading","summary":"Lake, B. M., Ullman, T. D., Tenenbaum, J. B., \u0026amp; Gershman, S. J. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40, e253.\n You should read the entire target article (roughly the first 25 pages). The commentaries and response from the authors are optional.  Sign up to lead discussion here: http://bit.ly/lhm-signup","tags":null,"title":"Machines that learn like people","type":"docs"},{"authors":null,"categories":null,"content":"  Gershman, S., J. (2015). A unifying probabilistic view of associative learning. PLoS Computational Biology, 11, e1004567.\n You should read the first ~half of the paper until the start of the temporal difference section (bottom of page 7). There’s a great youtube talk-form of this paper from Sam Gershman that is really helpful for understanding the paper. The part for the first half of the paper ends at ~35 minutes. You can watch this instead of or in addition to the reading if you like.\n Your goal should be to understand why this model is different from the Rescorla-Wagner model that you learned about the start of the semester, and be able to talk about their relative merits, and how they relate to higher-level ideas about frameworks for thinking about learning.\n   Gershman, S., J., \u0026amp; Niv, Y. (2012). Exploring a latent cause theory of classical conditioning. Learning \u0026amp; Behavior, 40, 255—268.\n You should understand what motivated this model (relative to classical conditioning models), the phenomena described, and why the model accounts for them.  The primary goal this week is to talk about how this framing of the learning problem is different from prior models of classical conditioning, what it buys us, and how compelling you find this account.\n","date":1603152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"2c7b8d86fe31121096798219ca0a6727","permalink":"https://dyurovsky.github.io/learning-humans-machines/reading/15-reading/","publishdate":"2020-10-20T00:00:00Z","relpermalink":"/learning-humans-machines/reading/15-reading/","section":"reading","summary":"Gershman, S., J. (2015). A unifying probabilistic view of associative learning. PLoS Computational Biology, 11, e1004567.\n You should read the first ~half of the paper until the start of the temporal difference section (bottom of page 7). There’s a great youtube talk-form of this paper from Sam Gershman that is really helpful for understanding the paper. The part for the first half of the paper ends at ~35 minutes.","tags":null,"title":"Bayesian associative learning","type":"docs"},{"authors":null,"categories":null,"content":"  Colunga, E., \u0026amp; Smith, L. B. (2005). From the lexicon to expectations about kinds: a role for associative learning. Psychological Review, 112, 347—382.\n Read the introduction, Experiments 1-3, and the discussion and conclusion. Your goal should be to understand what the phenemon being modeled is, how the model works, and what the basic results are.   Kemp, C., Perfors, A., \u0026amp; Tenenbaum, J. B. (2007). Learning overhypotheses with hierarchical Bayesian models.. Developmental Science, 10, 307—321.\n You can skip the section on ontological kinds. Your goal should again be to understand what the model is doing and why it produces the results it does.  The primary goal this week is to think about the relationship between these two models. How are they the same? How are they different? Are there reasons to prefer one to the other? Are there some things that one does better than the other?\n","date":1602115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602115200,"objectID":"a21d1f023b9fb481f6f6ab263dd324cc","permalink":"https://dyurovsky.github.io/learning-humans-machines/reading/12-reading/","publishdate":"2020-10-08T00:00:00Z","relpermalink":"/learning-humans-machines/reading/12-reading/","section":"reading","summary":"Colunga, E., \u0026amp; Smith, L. B. (2005). From the lexicon to expectations about kinds: a role for associative learning. Psychological Review, 112, 347—382.\n Read the introduction, Experiments 1-3, and the discussion and conclusion. Your goal should be to understand what the phenemon being modeled is, how the model works, and what the basic results are.   Kemp, C., Perfors, A., \u0026amp; Tenenbaum, J. B. (2007). Learning overhypotheses with hierarchical Bayesian models.","tags":null,"title":"Models at different levels","type":"docs"},{"authors":null,"categories":null,"content":"  Marcus, G. F., Vijayan, S., Rao, S. B., \u0026amp; Vishton, P. M. (1999). Rule learning by seven-month-old infants. Science, 283, 77—80. Also read the responses.\n Your goal here should be to understand Marcus et al.’s experimental paradigm and why they think it means that human cognition cannot operate the way that neural networks do. Do you agree? Do you find the criticisms compelling?   McClelland, J. L., \u0026amp; Plaut, D. C. (1999). Does generalization in infant learning implicate abstract algebra-like rules?. Trends in Cognitive Sciences, 3, 166—168. Also read the Marcus response.\n This is a more detailed objection to the Marcus (1999) argument. Make sure you understand what they are suggesting that networks can learn, and also why Marcus is not impressed.  Marcus, G. (2018). Deep learning: A critical appraisal. arXiv preprint.\n Neural networks in 2018 are much more impressive than they were in 1999. And yet, Marcus is still concerned. As you read this, think about whether the same arguments are being made here as in his 1999 paper. Are previous arguments refuted? Are there new compelling arguments?  ","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600905600,"objectID":"84eea7e2ac7daba0422f51df6e252e34","permalink":"https://dyurovsky.github.io/learning-humans-machines/reading/08-reading/","publishdate":"2020-09-24T00:00:00Z","relpermalink":"/learning-humans-machines/reading/08-reading/","section":"reading","summary":"Marcus, G. F., Vijayan, S., Rao, S. B., \u0026amp; Vishton, P. M. (1999). Rule learning by seven-month-old infants. Science, 283, 77—80. Also read the responses.\n Your goal here should be to understand Marcus et al.’s experimental paradigm and why they think it means that human cognition cannot operate the way that neural networks do. Do you agree? Do you find the criticisms compelling?   McClelland, J. L.","tags":null,"title":"Limits to connectionism","type":"docs"},{"authors":null,"categories":null,"content":"  Ramscar, M., Yarlett, D., Dye, M., Denny, K., \u0026amp; Thorpe, K. The Effects of feature-label-order and their implications for symbolic learning. Cognitive Science, 34, 909-957.\n This is a challenging paper—both the theory and the model are pretty dense. If you don’t understand all of the distinctions that the authors are drawing between reference and prediction don’t worry too much about that (section 1). But make sure you understand sections 2-8. The rest of the paper is optional.   Smith, L. B. Learning how to learn words: An associative crane. In R. Golinkoff, \u0026amp; K. Hirsh-Pasek (Eds.), Breaking the word learning barrier (pp. 51-80). Oxford: Oxford University Press.\n This paper should be a little bit easier. Make sure you understand the primary argument about the difference between cranes and skyhooks as explanations, and how that maps onto theories of word learning according to Smith.  ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"13bbf5064f8e7d91a62ab29bec4d6a2c","permalink":"https://dyurovsky.github.io/learning-humans-machines/reading/04-reading/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/learning-humans-machines/reading/04-reading/","section":"reading","summary":"Ramscar, M., Yarlett, D., Dye, M., Denny, K., \u0026amp; Thorpe, K. The Effects of feature-label-order and their implications for symbolic learning. Cognitive Science, 34, 909-957.\n This is a challenging paper—both the theory and the model are pretty dense. If you don’t understand all of the distinctions that the authors are drawing between reference and prediction don’t worry too much about that (section 1). But make sure you understand sections 2-8.","tags":null,"title":"How far can simple associative learning get you?","type":"docs"},{"authors":null,"categories":null,"content":"   Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph. More text in the next paragraph. Always use empty lines between paragraphs.  Some text in a paragraph.\nMore text in the next paragraph. Always use empty lines between paragraphs.\n  *Italic* _Italic_ Italic  **Bold** __Bold__ Bold  # Heading 1  Heading 1   ## Heading 2  Heading 2   ### Heading 3  Heading 3   (Go up to heading level 6 with ######)    [Link text](http://www.example.com)  Link text  ![Image caption](/path/to/image.png)    `Inline code` with backticks  Inline code with backticks  \u0026gt; Blockquote   Blockquote\n  - Things in - an unordered - list * Things in * an unordered * list  Things in an unordered list   1. Things in 2. an ordered 3. list 1) Things in 2) an ordered 3) list Things in an ordered list   Horizontal line --- Horizontal line *** Horizontal line\n     Math Markdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n    Type… …to get    Based on the DAG, the regression model for estimating the effect of education on wages is $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or $\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$. Based on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).    To put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math: $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ But now we just use computers to solve for $x$. …to get…\n The quadratic equation was an important part of high school math:\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\nBut now we just use computers to solve for \\(x\\).\n Because dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n Tables There are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: Caption goes here …to get…\n Caption goes here  Right Left Center Default    12 12 12 12  123 123 123 123  1 1 1 1    For pipe tables, type…\n| Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Table: Caption goes here …to get…\n Caption goes here  Right Left Default Center    12 12 12 12  123 123 123 123  1 1 1 1     Footnotes There are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags]. [^1]: This is a note. [^note-on-dags]: DAGs are neat. And here\u0026#39;s more of the document. …to get…\n Here is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n  This is a note.↩︎   DAGs are neat.↩︎     You can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!] …to get…\n Causal inference is neat.1\n  But it can be hard too!↩︎      Front matter You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; --- You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n--- title: \u0026quot;My cool title: a subtitle\u0026quot; --- If you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n--- title: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39; ---  Citations One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib --- Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib csl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot; --- Some of the most common CSLs are:\n Chicago author-date Chicago note-bibliography Chicago full note-bibliography (no shortened notes or ibids) APA 7th edition MLA 8th edition  Cite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n    Type… …to get…    Causal inference is neat [@Rohrer:2018; @AngristPischke:2015]. Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).  Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1]. Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).  Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018]. Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).  @AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees. Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.    After compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n Angrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n   Other references These websites have additional details and examples and practice tools:\n CommonMark’s Markdown tutorial: A quick interactive Markdown tutorial. Markdown tutorial: Another interactive tutorial to practice using Markdown. Markdown cheatsheet: Useful one-page reminder of Markdown syntax. The Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.   ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/learning-humans-machines/resource/markdown/","section":"resource","summary":"Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms  Document: A Markdown file where you type stuff\n Chunk: A piece of R code that is included in your document. It looks like this:\n```{r} # Code goes here ``` There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n Knit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n   Add chunks There are three ways to insert chunks:\n Press ⌘⌥I on macOS or control + alt + I on Windows\n Click on the “Insert” button at the top of the editor window\n Manually type all the backticks and curly braces (don’t do this)\n   Chunk names You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk} # Code goes here ```  Chunk options There are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE} # Code goes here ``` The most common chunk options are these:\n fig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures echo=FALSE: The code is not shown in the final document, but the results are message=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted warning=FALSE: Any warnings that R generates are omitted include=FALSE: The chunk still runs, but the code and results are not included in the final document  You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n Inline chunks You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE} avg_mpg \u0026lt;- mean(mtcars$mpg) ``` The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon. … would knit into this:\n The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n  Output formats You can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot; output: html_document: default pdf_document: default word_document: default You can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n--- title: \u0026quot;My document\u0026quot; author: \u0026quot;My name\u0026quot; date: \u0026quot;January 13, 2020\u0026quot; output: html_document: toc: yes fig_caption: yes fig_height: 8 fig_width: 10 pdf_document: latex_engine: xelatex # More modern PDF typesetting engine toc: yes word_document: toc: yes fig_caption: yes fig_height: 4 fig_width: 5 ---  ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"https://dyurovsky.github.io/learning-humans-machines/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/learning-humans-machines/resource/rmarkdown/","section":"resource","summary":"Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"  Here’s your roadmap for the semester!\n  Readings should be completed before each class session  Assignments are due by 11:59 PM on the day they are due  Class materials (slides, in-class activities, etc.) will be added on the day of class     Simple Neural Networks Reading Assignment Class   September 1  Learning in Humans and Machines        September 3  R, RStudio, and Github        September 8  Associative Learning        September 10  How far can simple associative learning get you?       September 15  Perceptrons        September 16 Implementing the Rescorla-Wagner Model        September 17  Multi-layer Networks        September 22  Backpropagation details        September 24  Limits to connectionism       September 29  Recurrent neural networks        October 9 Perceptrons and backpropagation         Bayesian learning Reading Assignment Class   October 1  Basics of Bayesian Inference        October 6  Learning by Bayesian inference        October 8  Models at different levels       October 13  Rational analysis        October 15  Inference by sampling        October 20  Bayesian associative learning       October 21 The number game        October 22  Comparing models        October 27  Machines that learn like people       November 6 Markov chain Monte Carlo         Learning from other people Reading Assignment Class   October 29  Learning from teaching        Nov 3  What makes a good teacher?        Nov 5  Rational speech acts        Nov 6 Project Proposal         Nov 10  Indirectly learning from language       Nov 12  Iterated learning        Nov 17  Community effects on learning from others       Nov 19  The structure in language        Nov 24  Modern language models         November 25 Latent semantic analysis         Nov 26 Thanksgiving - No class         Dec 1  Why training data matters          Projects Reading Assignment Class   Dec 3  Project Presentations         Dec 10  Project Presentations         Dec 8  Wrap up         December 15 Final Project due          ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"https://dyurovsky.github.io/learning-humans-machines/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/schedule/","section":"","summary":"Here’s your roadmap for the semester!\n  Readings should be completed before each class session  Assignments are due by 11:59 PM on the day they are due  Class materials (slides, in-class activities, etc.) will be added on the day of class     Simple Neural Networks Reading Assignment Class   September 1  Learning in Humans and Machines        September 3  R, RStudio, and Github        September 8  Associative Learning        September 10  How far can simple associative learning get you?","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"  Course objectives Important pep talk! Course structure Seminar Commentaries Homeworks Final Project  Course materials R and RStudio Github Online help  Course policies Course evaluation and evolution Office hours Accommodations for Students with Disabilities: Respect in the Classroom Health \u0026amp; Well-Being Academic honesty  Assignments and grades   Instructor  Dr. Dan Yurovsky  Zoom Office  Tuesday 4:30-6:40 PM  yurovsky@cmu.edu  @danyurovsky   Course details  Tuesday/Thursday  September 15–April 22, 2020  1:30–2:50 PM  Zoom Room   Contacting me E-mail is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours (really), but also remember that life can be busy and chaotic for everyone (including me!), so if I don\u0026rsquo;t respond to your e-mail right away, don\u0026rsquo;t worry!\n  Course objectives By the end of this course, you will (1) have an understanding of formal frameworks used to characterize human learning, (2) understand how these are related to methods used in machine learning, (3) be able to implement these theories in working code, and (4) be able to apply these methods to new problems.\nSpecifically, you’ll be able to:\nArticulate a formal definition of what it means to “learn,” and why this can be different across different settings Describe (some) of what is known empirically about how humans learn Implement formal models of human learning from several different frameworks in the R programming language Explain how these models relate to models developed in machine learning Characterize how “good” a model is and the ways in which it is better or worse than other models of the same phenomenon.   Important pep talk! I promise you can succeed in this class.\nLearning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2—made this wise observation:\n It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n Even experienced programmers and evaluators find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n Course structure Because the goal for the course is for you to both learn theory and be able to apply it in practice, you’ll be learning through three primary modes: (1) short lectures, (2) hands-on programming assignments, and (3) seminar discussions with your classmates.\nThroughout each of the units, I will present lectures to familiarize you with the broad strokes of the important ideas, and to scaffold your ability to learn from the papers you will read and the assignments you will complete. I will generally lecture more at the start of the course than at the end. All lectures will be available online after class is finished.\nSeminar For the first unit, I will lead the seminar discussions that are interspersed with the lectures in order to set the tone for the course and to establish my expectations for the goals of seminar and present one way of trying to accomplish them. After that, students will be asked to lead the seminars in pairs or small groups. Before you lead a seminar, I’ll meet with you help you plan a successful discussion.\nRegardless of whether you are leading discussion, I expect everyone in the class to be an active participant–you’ll have a lot more fun, and learn a whole lot more if you engage with your classmates’ ideas while we are all together. I know that this is hard to do, especially in the context of a remote class, and that you may be subject to circumstances that make it particularly challenging. For this reason, we will also have a Piazza forum where you can join in the discussion asynchronously. If you are unable to attend seminar, please make sure you contribute significantly here.\n Commentaries To help seed seminar discussion, you will be asked to submit a short commentary to the Piazza forum. Please submit your commentary by midnight day before seminar\nA commentary might take one of the following forms:\nDisagreement. Mention a claim that doesn’t seem right to you.\n Extension. Describe how the work could be usefully extended. Is there another way that the model could be tested? How might the model be revised to handle a broader range of cognitive phenomena?\n Criticism. List at least one flaw or limitation with the approach presented.\n Connection. Draw a connection between the reading and something else that you know about (e.g. something that we’ve discussed in a previous class, or that you learned about elsewhere).\n  Each commentary should not be long—one paragraph is typical. You can write up to half a page, but writing three thoughtful sentences is sufficient.\nThe presenters for the day will be reading your commentaries before class, so feel free to add a postscript with questions or thoughts about what you’d like to discuss during class. For example, if the reading made some point that was confusing but that you’d like to understand, include a note to this effect and we’ll try to address it during class.\n Homeworks Each unit will have a two hands-on programming assignments where you will implement the theoretical ideas we are discussing. The best way to develop an intuition for how models work and what they can (and can’t) do is to build them yourself. You are welcome to work on these together with other students, but you must turn in your own work\n Final Project As a capstone for the class, you will complete a final project. Your project can take one of several forms. You can develop a new model of some aspect of cognition, run a small experiment to test an existing computational model, implement an existing model and apply it to a new data set, or write a paper on some topic related to the class. Working on an existing research project is fine as long as this project matches the spirit of the class. You are encouraged to work in pairs, but working on your own is also fine.\nThe project includes four components:\nProposal. Submit no more than one page describing the question you plan to explore and the method you will use. Your proposal is due on October 30.\n Check in. You will meet with me during the week of November 9 to discuss how your project is going.\n Presentation. At the end of the semester you will briefly present your project. Your presentation should be about 10 minutes long, and you are encouraged to use visual aids (e.g. slides, figures drawn on the zoom board).\n Writeup.\n    Course materials All of the materials you will need for this course–both the readings and the software will be made freely available to you. Any articles, book chapters, or other materials I will ask you to read will be linked on the course website from the related week. Each will be accompanied by a short reading guide that will help you to know what to focus on when you read.\nR and RStudio You will do all of your programming in the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all of the computation, while RStudio provides a nice interface for running R code.\n Github In order to access and submit the assignments for the course, we will use GitHub Classroom. GitHub is a cloud-based version control system that is extremely popular in both academia and industry. You will likely find it more challenging to use at first than canvas, but you’ll learn skills that will generalize beyond the course.\n Online help Programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nIn addition to asking me and your classmates for help, there are tons of online resources to help you with this. One of the most important is StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions).\n  Course policies Be nice. Be honest. Try your best. Don’t cheat.\nWe will also follow the Carnegie Mellon University code of conduct.\nCourse evaluation and evolution This syllabus reflects a plan for the semester. Deviations may become necessary as the semester progresses.\nBecause this is a new class, there will be inevitable bumps along the way. Please be patient with me as I get everything ironed out!\nI want to make this the best class I can, and I’d love your help to help to do that. To facilitate this, I have a couple requests:\nIn the class link next to every class, you will find a link to an anonymous Google Form with a few quick questions asking about the clearest and most confusing things from that day. Please fill this out regularly. It will be hard to remember, but it’s extraordinarily helpful for me.\n At some point in the middle of the semester, someone from CMU’s Eberly Center for Teaching Excellence \u0026amp; Educational Innovation will come and run a 25 minute Early course feedback focus group, where I’ll step out and you’ll all talk to them about the class. They’ll give me anonymized feedback about what is going well, and what I should do to help you learn better.\n  Also, please take the time to fill out the official CMU course evaluation at the end of the semester!\n Office hours Please watch this video:\n Office hours are set times dedicated to all of you. This means that I will be sitting on Zoom (wistfully) waiting for you to come by with whatever questions you have. This is the best and easiest way to find me outside of class and the best chance for discussing class material and concerns. Please come by!\nOutside of regularly scheduled office hours, send me an email and I will be very happy to meet with you.\nThis can be a difficult class. Do not suffer in silence! Come talk to me!\n Accommodations for Students with Disabilities: If you have a disability and are registered with the Office of Disability Resources, I encourage you to use their online system to notify me of your accommodations and discuss your needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them.\n Respect in the Classroom It is my intent to present materials and activities that are respectful to the diverse backgrounds and perspectives of students in the classroom. You may feel free to let me know ways to improve the effectiveness of the course for you personally or for other students or student groups. If you feel uncomfortable discussing this with your instructor, you may voice your concerns to the Chair of the Department of Psychology Diversity and Inclusion (D\u0026amp;I) Committee, Jessica Cantlon. Dr. Cantlon and the D\u0026amp;I Committee are available to hear your concerns related to respect for diversity for any class you are taking in the Department of Psychology.\n Health \u0026amp; Well-Being Take care of yourself. Maintaining a healthy lifestyle via good nutrition, exercise, rest, and relaxation will help you achieve your goals and cope with stress. All of us benefit from support during times of struggle. There are many helpful resources available, so please reach out to us if you need help connecting with them on campus or virtually. Asking for support sooner rather than later is almost always helpful. If you or anyone you know experiences academic stress, difficult life events, or feelings of anxiety or depression, please seek support. Counseling and Psychological Services (CaPS) is available online and at 412-268-2922 or via. Consider reaching out to a friend, faculty member, or family member you trust for assistance with getting connected to the support that can help.\n Academic honesty Cheating and plagiarism are defined in the CMU Student Handbook, and include (1) submitting work that is not your own for assignments or exams; (2) copying ideas, words, or graphics from a published or unpublished source without appropriate citation; (3) submitting or using falsified data; and (4) submitting the same work for credit in two courses without prior consent of both instructors. Any student who is found cheating or plagiarizing on any work for this course will receive a failing grade for that work. Further action may be taken if necessary, including a report to the dean.\n  Assignments and grades Descriptions for all of the assignments will be posted on the on the assignments page.\n   Assignment Points Percent    Homeworks (6 x 12) 72 49%  Participation 15 10%  Commentaries 15 10%  Project presentation 15 10%  Final project 30 20%  Total 147 —        Grade Range Grade Range    A 93–100% C 73–76%  A− 90–92% C− 70–72%  B+ 87–89% D+ 67–69%  B 83–86% D 63–66%  B− 80–82% D− 60–62%  C+ 77–79% F \u0026lt; 60%      ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603912312,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"https://dyurovsky.github.io/learning-humans-machines/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/syllabus/","section":"","summary":"Course objectives Important pep talk! Course structure Seminar Commentaries Homeworks Final Project  Course materials R and RStudio Github Online help  Course policies Course evaluation and evolution Office hours Accommodations for Students with Disabilities: Respect in the Classroom Health \u0026amp; Well-Being Academic honesty  Assignments and grades   Instructor  Dr. Dan Yurovsky  Zoom Office  Tuesday 4:30-6:40 PM  yurovsky@cmu.edu  @danyurovsky   Course details  Tuesday/Thursday  September 15–April 22, 2020  1:30–2:50 PM  Zoom Room   Contacting me E-mail is the best way to get in contact with me.","tags":null,"title":"Syllabus","type":"page"}]