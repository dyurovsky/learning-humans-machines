[{"authors":["dan"],"categories":null,"content":"Dan Yurovsky is an assistant professor at the Department of Psychology at Carnegie Mellon University, researching language acquisition and cognitive development.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599770768,"objectID":"bc34d76bc3e159b0c10dc7a9fb6aef07","permalink":"/learning-humans-machines/authors/dan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/authors/dan/","section":"authors","summary":"Dan Yurovsky is an assistant professor at the Department of Psychology at Carnegie Mellon University, researching language acquisition and cognitive development.","tags":null,"title":"Dan Yurovsky","type":"authors"},{"authors":null,"categories":null,"content":"  Homeworks Final project   Homeworks To practice thinking about, evaluating, and implementing models, you will complete a series of homework assignments roughly every-other week. You may (and should!) work together on the homeworks, but you must turn in your own answers.\n Final project At the end of the course, you will demonstrate your knowledge There is no final exam. This project is your final exam.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599770768,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"/learning-humans-machines/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/assignment/","section":"assignment","summary":"  Homeworks Final project   Homeworks To practice thinking about, evaluating, and implementing models, you will complete a series of homework assignments roughly every-other week. You may (and should!) work together on the homeworks, but you must turn in your own answers.\n Final project At the end of the course, you will demonstrate your knowledge There is no final exam. This project is your final exam.\n ","tags":null,"title":"Assignment details","type":"docs"},{"authors":null,"categories":null,"content":"  I will post all the materials (slides, example code, in-class activities, etc.) from each class session on the day of the class. I typically don’t post the slides before class, but they will be posted after.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599770768,"objectID":"108da05078d325a5a1f01a1ff2583053","permalink":"/learning-humans-machines/class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/class/","section":"class","summary":"I will post all the materials (slides, example code, in-class activities, etc.) from each class session on the day of the class. I typically don’t post the slides before class, but they will be posted after.","tags":null,"title":"Class details","type":"docs"},{"authors":null,"categories":null,"content":" Completing the readings before each class is an essential part of this course. Your seminar participation is a substantial part of your grade, so please completed readings before each class session.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1599770768,"objectID":"40fcd2da3bf2dc718a2fe044c31cdc56","permalink":"/learning-humans-machines/reading/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/reading/","section":"reading","summary":"Completing the readings before each class is an essential part of this course. Your seminar participation is a substantial part of your grade, so please completed readings before each class session.","tags":null,"title":"Reading policy","type":"docs"},{"authors":null,"categories":null,"content":"  The Rescorla-Wagner Model Simple conditioning Extinction Blocking Conditioned inhibition   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw1.Rmd, and please knit the Markdown file in your completed submission.\nThe Rescorla-Wagner Model The Rescorla-Wagner model, developed by Robert Rescorla and Allen Wagner in 1972, was extremely influential at the time of its publication because it was able to explain several puzzling findings in Pavlovian condition, especially the phenomenon of blocking. It has since been extended by researchers working in Reinforcement learning to account for a number of other interesting phenomena. It is also the basis of the delta rule used for training simple neural networks, as you’ll see later on the course.\nYou’ll work with the same simplified version of the model that you saw in class. The model describes the change in strength associated with a conditioned stimulus (\\(\\Delta V\\)) with this equation:\n\\[\\Delta V = \\alpha \\cdot \\left(\\lambda - V_{total}\\right)\\]\nProblem 1: Describe, in words, what each symbol in this equation means and how it is related to learning (1 point). Now let’s turn this equation into R code. You’ll write a function called rw_delta_v that computes \\(\\Delta V\\) according to the Rescorla-Wagner equation. It should take all three of relevant parameters(\\(\\alpha\\), \\(\\lambda\\), and \\(V_{total}\\)). It should return the amount that the target weight will change.\n Problem 2: Write rw_delta_v (1 point). You’ll use the stub in the R Markdown file in the GitHub Repository that looks like this:\nrw_delta_v \u0026lt;- function(Vtotal, alpha = .1, lambda = 1) { } One thing to notice is that function parameters in R can have defaults specified. If you don’t pass in a value for that parameter, it will get the default value inside the function.\n  Simple conditioning To get started, you’ll simulate of simple conditioning experiment in which someone experiences 10 trials of positive reinforcement in response to a conditioned stimulus. You’ll want to produce a plot of the strength of the conditioned stimulus over the course of these 10 trials so you can see the changes that the model predicts.\nThere are lots of ways of setting up this experiment in R, and you’re welcome to do it however you like. In case you’d like a hand to get started, he’s one strategy:\n Make a tibble with 2 columns: trial and V. The trial column will have the values \\(0\\) through \\(10\\), and the V column will start as all \\(0\\)s.\n Write a for loop that iterates over the numbers \\(1\\) through \\(10\\)—these will index into the rows of your tibble. Set the value of the V column in each row to the result of calling your rw_delta_v function on the value of V in the previous row.\n Make a plot with trial on the x-axis and V on the y-axis. You can make one plot for all four of your simulations if you’re feeling comfortable with the tidyverse, or 4 separate plots.\n  Problem 3: Run 4 simulations of the experiment–try 2 different levels of \\(\\alpha\\) and 2 different levels of \\(\\lambda\\). What effect does these parameters have on the model predictions? (3 points).   Extinction Now let’s see what happens if we take the reinforcer away. Set up a simulation where the participant is exposed to 10 trials in of positive reinforcement in response to a conditioned stimulus, and then 10 trials in which they get no reinforcement (\\(\\lambda = 0\\)). Then make a plot of \\(V\\) over the course of the experiment.\nProblem 4: Try the same parameter values that you used above in this new experiment. How does the extinction curve depend on \\(\\alpha\\) and \\(\\lambda\\)? (2 points). This should be a fairly straightforward extension of the code you wrote for the last Problem. The critical thing will be to make sure that you are using the right value of the \\(\\lambda\\) parameter on each trial—remember, no reinforcer should have no reward and thus \\(\\lambda = 0\\).\n  Blocking Now you’re ready to test Rescorla-Wagner’s ability to account for the Blocking phenomenon. In this simulation, you’ll have two cues–x and y. For the first 10 trials, the simulated participant will be exposed to cue x and be positively reinforced. Then, on the subsequent 30 trials, both x and y will be present and the participant will get reinforced.\nOne way to make this simulation work is to replace the V column with two new columns: Vx and Vy. And then include two more columns in your tibble, x_present and y_present, which indicate whether each cue is present on each trial. You then want to make sure that you simulate updating the weight for all cues that are present \\(\\Delta V\\). And make sure that \\(V_{total}\\) has the right value on each trial!\nProblem 5: Implement the blocking experiment described above and make a plot of the weights of each of the two cues over the course of the 40 trials. You can pick any values for \\(\\alpha\\) and \\(\\lambda\\) (3 points).   Conditioned inhibition Finally, you’re ready to simulate a phenomenon we talked about in class but that you didn’t see directly: Conditioned inhibition. The setup is similar to blocking–first one cue (x) is presented and reinforced, and then x and another cue y appear together. But, this time their combination is not reinforced. As a result, people (and the model) learn a negative value for y. Intuitively, y is the reason that x appearing did not lead to a positive reinforcer.\nProblem 6: Implement the a conditioned inhibition experiment just like your blocking experiment above, except this time without reinforcement on the 30 trials with both stimuli. Make a plot of the values of the two cues over the course of the experiment. Pick any values for \\(\\alpha\\) and \\(\\lambda\\) (2 points).   ","date":1599609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599770768,"objectID":"4ba2d603b0ca245a3bd1a69b53c8f8d1","permalink":"/learning-humans-machines/assignment/01-rescorla-wagner/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/learning-humans-machines/assignment/01-rescorla-wagner/","section":"assignment","summary":"The Rescorla-Wagner Model Simple conditioning Extinction Blocking Conditioned inhibition   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw1.Rmd, and please knit the Markdown file in your completed submission.","tags":null,"title":"Homework 1","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Code Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Code McMurray (2007) model – html and Rmd.\n Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Papers McMurray, B. (2007). Defusing the childhood vocabulary explosion. Science, 317, 631.\n ","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599770768,"objectID":"ac47977a15b3902ca3402f61e5bf9df2","permalink":"/learning-humans-machines/class/01-class/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/learning-humans-machines/class/01-class/","section":"class","summary":"Slides Code Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Code McMurray (2007) model – html and Rmd.\n Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today?","tags":null,"title":"Learning in humans and machines","type":"docs"},{"authors":null,"categories":null,"content":"  Introduction Perceptrons Multi-layer networks Using the neuralnet package   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw2.Rmd, and please knit the Markdown file in your completed submission.\nIntroduction Neural networks were a revolution in the scientific study of cognitive neuroscience, spawning a large body of work investigating the computational properties of systems designed to model the operation of the brain’s basic computational units (Rosenblatt, 1958. These perceptrons varied along a number of dimensions, but all of them had the same critical flaw: they could not learn non-linear combinations of inputs, leading to their failure on even simple problems like Exclusive Or (XOR; Minsky \u0026amp; Papert, 1967).\nIn the 1980s, work by David Rumelhart and his colleagues rekindled the field’s interest in neural networks by devising an algorithm by which these models could learn non-linear combinations of input and develop genuinely interesting and surprisingly representations of their input (Rumelhart, Hinton, \u0026amp; Williams, 1986). This lead to an explosion of work in artificial neural networks in the following decades, and also, following Marr’s perspective, a reconsideration of what these models were intended to describe (e.g. the neurons in the networks need not map on to neurons in the brain).\nIn this assignment you will first implement simple one-layer perceptrons that learn with the perceptron learning rule. You’ll show that these can learn several logical functions: AND, OR, and NOT. But they cannot learn XOR.\nYou will then build a very simple multi-layer network that uses backpropagation to learn XOR. You’ll finally use the neuralnet package to solve this same problem, and then use it to build a digit classifier using a small version of the MNIST dataset\n Perceptrons Your first goal will be to train a perceptron to solve logical AND. I’ve provided a set of stub functions that scaffold one way of doing this. The idea is to approximate a sort of pseudo-object oriented structure using a named list. This is overkill for just this simple perceptron, but you’ll find that it extends easily to a backprop network.\nThis object defined in the perceptron function. A perceptron is a list that has 5 elements:\nA tibble of inputs\n A list of target ys for those inputs\n A list of outputs for the last run of the network corresponding to these inputs\n A list of activations that occur after applying the sigmoid function to those outputs\n A list of weights – 3 in total. Two that connect from the input nodes (x1, x2), and one that connects from a bias node (1) to the output node (y).\n  This list will track the state of the perceptron as it goes through the training function you write (train_perceptron). You can write this function as two nested for loop, the outer one over iterations, and the inner one over examples. In each run of the inner loop, you will\nRun perceptron_feedforward over the training example.\n Run perceptron_feedback over that training example to update the weights.\n  In each run of the outer loop, you will run perceptron_feedforward and perceptron_feewdback on all of the examples, and then compute the error for that iteration and store it in the network, e.g. perceptron$errors[iteration] \u0026lt;- sq_error(...).You’ll need to make sure you define perceptron$errors as a list of the right length at the start of the train_perceptron function.\nIf you want to use these stubs, your plan of attack should be:\nWrite the helper functions sigmoid and sq_error\n Make the and_data tibble with columns x1, x2, and y that corresponds to the four possible input and output combinations for AND\n Write the perceptron_feedforward and perceptron_feedback functions\n Write train_perceptron\n Run train_perceptron on your AND data, and plot or otherwise summarize the change in errors over time and the final weights.\n  Problem 1: Fill out the stub helper functions sigmoid and sq_error (1 point)  Problem 2: Write code to train a perceptron to solve logical AND. You can use the stubs provided in the hw2.Rmd or write your own. Train the perceptron, and report back on the results. Did the error in the network go down over the course of training? What were the final weights? (3 points)  Problem 3: Use your functions to train the perceptron on OR, NOT X1, and XOR. Which problems did it succeed on. Which did it fail on? What were the final weights for each? Do they make sense? (2 points)   Multi-layer networks If everything went right, you will have discovered in Problem 3 that perceptrons cannot learn to solve non-linear classification problems like XOR. But with a hidden layer, we can fix this problem. You can use the same strategy you used for your perceptron to implement a 2-layer backpropagation network. You might find the backprop by hand example we did helpful for reference for the weight update formulas.\nProblem 4: Write code to train a 2-layer network to learn XOR. You should have 3 hidden layer nodes — 2 that take input from the input layer, and 1 bias node. After training, investigate the hidden layer nodes. What has the network learned? (3 points)   Using the neuralnet package The network you implemented is likely to be pretty inefficient. In practice, most implementations of neural networks use matrix multiplication to compute weights and outputs, which drastically speeds things up. For the last two problems, you’ll be using the neuralnet r package to investigate what these networks can learn in a more interesting problem.\nThe workhorse of the package is the neuralnet function which trains neural nets to solve problems. It uses formula notation just like lm and other standard statistical methods in R.\nProblem 5: Use the provided code to run neuralnet on your xor data, and the plot your neuralnet to see what the weights on each node are. Did it learn the same thing as the network you made from scratch? Now that you understand how this package works, you’ll use it to solve a more interesting classification problem. You’ll be learning to classify handwritten digits from the MNIST dataset that are a classic success story for modern neural networks. You’ll be working with just a small, scaled down subset of the real dataset — 100 examples of each of the digits 0-9. Here is the first example of each:\nYour goal will be to try out the neuralnet package to find what kinds of network structures matter for learning to classify these digits. The representation you’ll work with is a linearized version of the digits — imagine taking all of the rows of these figures and chaining them all into one long row. This means your network won’t have any ability to use the spatial structure of the images. But even with just the pixel values, you’ll find that you can do a fair bit better than chance (\\(\\frac{1}{10}\\)).\nThe neuralnet package uses what is called a one-hot encoding: The output layer will have ten nodes, each corresponding to a digit. The goal of the network when it reads say a \\(9\\) is for all of the output nodes except for the one corresponding to \\(9\\) to have no activation, and for the \\(9\\) node to have 1.\n Problem 6: Play around with the hidden layer structure of the network. You can specify how many units there are. For instance, using the argument hidden = 2 makes 2 hidden layer nodes (plus a bias). using the argument hidden = c(3,2) makes 2 hidden layers - the first having 3 units and the second having 2 (plus a bias). If the number of units gets too big you’ll find that the network crashes or fails to converge. But report back on at least 3 argument values that successfully ran. You can use the provided prediction_error function will compute how much error there is. Make sure you look at error both on the training set (mnist_train) and on a set of examples that it hasn’t been trained on (mnist_test). Is the network overfitting? What is it learning? Feel free to extend the prediction_error function to compute other metrics if you think they would be interesting.\n  ","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"a62ebcb9b73035c44fe4e9ec162a9a74","permalink":"/learning-humans-machines/assignment/02-networks/","publishdate":"2020-09-29T00:00:00Z","relpermalink":"/learning-humans-machines/assignment/02-networks/","section":"assignment","summary":"Introduction Perceptrons Multi-layer networks Using the neuralnet package   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw2.Rmd, and please knit the Markdown file in your completed submission.","tags":null,"title":"Homework 2","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1599091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599770768,"objectID":"9ae245330f59c57e6f62568d635c26e4","permalink":"/learning-humans-machines/class/02-class/","publishdate":"2020-09-03T00:00:00Z","relpermalink":"/learning-humans-machines/class/02-class/","section":"class","summary":"Slides Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"R, RStudio, and Github","type":"docs"},{"authors":null,"categories":null,"content":"  Introduction Modeling the number game The candidate hypotheses Modeling Inference Inferring hypotheses Predicting the next number.   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw3.Rmd, and please knit the Markdown file in your completed submission.\n Introduction An important problem in inferential learning is that the data available are almost always too sparse to unambiguously pick out the true hypothesis (see e.g. Quine, 1960). Human learners, however, appear to cope with this problem just fine, converging on a small set of possible hypothesis from very little data. In machine learning, this is known as few-shot learning, and is a real challenge for modern learning systems. One hypothesis for what helps human learners is that have a strong prior on the kinds of hypotheses that are likely and relevant, which makes the search problem much easier.\nIn this assignment, you’ll be working with a simple problem designed to demonstrate this phenomenon, and building a model that learns in a human-like way. The number game, developed by Josh Tenenbaum (2000), demonstrates the power of strong sampling for inductive inference. The key insight is that the probability that data is generated by a hypothesized process is proportional to the size of the set of possible data that the hypothesis could generate.\nIn the number game, you get as input one or more numbers between 1 and 100 generated by a computer program according to an unknown rule. Your job is to determine the most likely rule, or to make predictions about which numbers the program will generate next.\n Modeling the number game To figure out what hypothesis the computer is generating samples from, you can use Bayesian inference. To do this, you will use Bayes Rule to infer the posterior probability for each hypothesis:\n\\[ P\\left(H \\vert D\\right) \\propto P\\left(D \\vert H\\right)P\\left(H\\right)\\]\nTo specify the model, you need to specify a prior probability (\\(P\\left(H\\right)\\)) for each hypothesis, and a likelihood function that specified how probable the observed data are under that hypothesis (\\(P\\left(D \\vert H\\right)\\)).\nPrior: Following Tenenbaum (2000), we’ll say that hypotheses can be of two kinds: Mathematical and Interval. We’ll say that aprior, there is a \\(\\lambda\\) probability that the true hypothesis is in the set of Mathemtical hypotheses, and a (1-\\(\\lambda\\)) probability that the true hypothesis is in the set of Interval Hypotheses\nLikelihood: If we assume that the every number consistent with a particular hypothesis is equally likely to be generated by the machine, then if every number in \\(D\\) is in \\(H\\), the likelihood that set of numbers \\(D\\) is generated by the hypothesis \\(H\\) is the probability of choosing \\(\\lvert D \\rvert\\) numbers at random from the set of \\(\\lvert H \\rvert\\) numbers (\\(\\frac{1}{\\lvert H \\rvert \\choose \\lvert D \\rvert}\\)). You can compute this in R using the choose function. Otherwise, \\(H\\) cannot generate \\(D\\) and the likelihood is 0.\n The candidate hypotheses Your first goal is to write all of the candidate hypotheses that your model will consider. You don’t need to exhaustively include all of the hypotheses in the original Tenenbaum (2000) model, just the following\nMathematical hypotheses:\n even numbers\n odd numbers\n square numbers\n cube numbers\n prime numbers\n multiples of any number \\(n\\), \\(\\left(3 \\leq n \\leq 12\\right)\\)\n powers of any number \\(n\\), \\(\\left(2 \\leq n \\leq 10\\right)\\)\n  Interval hypotheses:\n decades (\\(\\left\\{1-10,\\; 10-20, \\;\\ldots \\right\\}\\))\n any range \\(1 \\leq n \\leq 100, n \\leq m \\leq 100, \\left\\{n-m\\right\\}\\)\n  One nice way of representing these hypotheses as tibbles with two columns: name which gives a human-readable description of the hypothesis, and set which is a list of all of the numbers 1—100 consistent with that hypothesis. You’ll first. write a function that generates each of these. For the multiple-set hypotheses (e.g. multiples of any number), you can make the function generate a multi-row tibble where each row is one of the sub-hypotheses.\nProblem 1: Define all of the hypotheses for the number game. You’re welcome to use the stubs I have provided in the template. (3 points). Now that you have each of the individual hypotheses, you can put them all together. A nice representation for this is one big tibble that has a row for all of the hypotheses you’ve generated. You’ll find the bind_rows function to be helpful here.\nThe last detail is that you want to be able to assign a prior probability appropriately to each of these hypotheses. Remember, all of the mathematical hypothesis together should have prior probability \\(\\lambda\\), and the interval hypotheses should make up the remainder of the prior (\\(1 - \\lambda\\)). You’ll want to divy up this probability equally within those respective sets.\n Problem 2: Write the function make_all_hypotheses which takes in a value for the parameter \\(\\lambda\\) and construct the full tibble of hypotheses. It should have four columns: name, set, type (mathematical or interval), and prior which have a value of each of all of the possible hypotheses. Hint: There should be 5,073 total (1 point).   Modeling Inference Now that you have all of your hypotheses, you need to write the likelihood function. And finally to compute the posterior probability of each hypothesis.\nProblem 3: Write the function compute_likelihood which takes in a set of numbers consistent with a hypothesis \\(H\\) , and a set of input numbers \\(D\\), and which returns the likelihood of generating that input set from that hypothesized number set (\\(P\\left(D \\vert H\\right)\\)) (1 point).  Problem 4: Write the function best_hypotheses which takes in a tibble of hypotheses (\\(H\\)), a set of input numbers \\(D\\), an optional number of hypotheses \\(n\\), and returns a tibble with the top \\(n\\) hypotheses for the data according to the posterior probability (\\(P\\left(H \\vert D\\right)\\)) as well as their corresponding posterior probability. You might find it helpful not to return every row of the original hypotheses tibble, but only their type (mathematical or interval), name, and posterior (2 points).   Inferring hypotheses Now you are ready to test the model! Using the default \\(\\lambda\\) value of \\(\\frac{4}{5}\\), you will get the top 5 best hypotheses for each of the following sets of data \\(D\\):\n \\(2\\)\n \\(2, 4, 8\\)\n \\(15, 22\\)\n \\(1, 2, 3, 4, 6\\)\n  Problem 5: Print out the top 5 best hypotheses according to the model for each of these values. Does the model predict what you think it should? Were there any surprises? (2 points)  Problem 6: Try changing the value of \\(\\lambda\\) to something smaller, like \\(\\frac{1}{5}\\). What effect does this have on the model’s predictions for the examples above? (1 point)   Predicting the next number. Now you will implement the final part of the model: Prediction about whether the computer is likely to generate a new number \\(d\u0026#39;\\) having observed all of the previous examples \\(D\\). To do this, you will use Bayesian model averaging. Instead of committing to the best hypothesis after observing \\(D\\), you will average together all of the hypotheses, weighing them by the posterior probability of them being true after observing \\(D\\).\n\\[ P\\left(d\u0026#39;\\right) = \\sum_{h \\in H}P\\left(d\u0026#39; | h\\right)P\\left(h | D\\right)\\]\nYou’ll just need to write one new function predict_value which will take in a tibble of hypotheses, a set of previously observed numbers, and a new target number. It should return the posterior probability of observing that number according to the formula above. You shouldn’t need any new helper functions here!\nTo check whether it works correctly, try these examples using the default value for \\(\\lambda\\):\n \\(D = 2, \\;d\u0026#39; = 12\\)\n \\(D = 2, 4, 8, \\;d\u0026#39; = 12\\)\n \\(D = 15, 22 \\; d\u0026#39; = 21\\)\n \\(D = 15, 22 \\; d\u0026#39; = 50\\)\n  Problem 7: Write the predict_value function and test it on the examples above. Did you get the output you expected? (2 points)   ","date":1602633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602633600,"objectID":"bad2d3721a45185b01e6a26bce1506df","permalink":"/learning-humans-machines/assignment/03-bayes/","publishdate":"2020-10-14T00:00:00Z","relpermalink":"/learning-humans-machines/assignment/03-bayes/","section":"assignment","summary":"Introduction Modeling the number game The candidate hypotheses Modeling Inference Inferring hypotheses Predicting the next number.   h4 { margin-top: 10px; margin-bottom: 10px; padding-top: 10px; padding-bottom: 10px; border-color: #d45026; border-style: solid; background-color: rgba(212, 80, 38, 0.2); font-weight: normal; }  Getting your assignment:  You can find template code for your submission here at this GitHub Classroom link. All of the code you write you should go in hw3.","tags":null,"title":"Homework 3","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Papers Rescorla R. A., \u0026amp; Wagner A. R. (1972). A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In A. H. Black \u0026amp; W. F. Prokasy (Eds.), Classical Conditioning II: Current Research and Theory. New York: Appleton Century Crofts.\n ","date":1599523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599770768,"objectID":"a0c01f3889200201f6df018341b3db8f","permalink":"/learning-humans-machines/class/03-class/","publishdate":"2020-09-08T00:00:00Z","relpermalink":"/learning-humans-machines/class/03-class/","section":"class","summary":"Slides Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Associative learning","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599770768,"objectID":"ac7533426a3114e90edf09c0635b197a","permalink":"/learning-humans-machines/class/04-class/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/learning-humans-machines/class/04-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"How far can simple associative learning get you?","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Papers Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological review, 65, 386—408.\n ","date":1600128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600128000,"objectID":"0ddf3e2b3a9f5347c88f83033fb01ff2","permalink":"/learning-humans-machines/class/05-class/","publishdate":"2020-09-15T00:00:00Z","relpermalink":"/learning-humans-machines/class/05-class/","section":"class","summary":"Slides Recording Clearest and most confusing things Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Perceptrons","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things Youtube videos Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Youtube videos 3Blue1BRown - What is backpropogation really doing?\n3Blue1BRown - Backpropogation calculus\n Papers McClelland, J. L., \u0026amp; Rogers, T. T. (2003). The parallel distributed processing approach to semantic cognition. Nature Reviews Neuroscience, 4, 310—322.\nMcClelland, J. L., \u0026amp; Rumelhart, D. E. (1981). An interactive activation model of context effects in letter perception: I. An account of basic findings. Psychological Review, 88, 375—407.\nRumelhart, D. E., Hinton, G. E., \u0026amp; Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323, 533—536.\n ","date":1600300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600300800,"objectID":"119022d4a0cb3ad45bc292e062e9e05a","permalink":"/learning-humans-machines/class/06-class/","publishdate":"2020-09-17T00:00:00Z","relpermalink":"/learning-humans-machines/class/06-class/","section":"class","summary":"Slides Recording Clearest and most confusing things Youtube videos Papers   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Multi-layer networks","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Clearest and most confusing things Backpropopagation example   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n Backpropopagation example XOR by hand\n ","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600905600,"objectID":"026700f4d4068083b37f98f89edf4682","permalink":"/learning-humans-machines/class/07-class/","publishdate":"2020-09-24T00:00:00Z","relpermalink":"/learning-humans-machines/class/07-class/","section":"class","summary":"Slides Recording Clearest and most confusing things Backpropopagation example   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Backpropagation details","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600905600,"objectID":"a696c93d764e2bfb447300660fcf2be7","permalink":"/learning-humans-machines/class/08-class/","publishdate":"2020-09-24T00:00:00Z","relpermalink":"/learning-humans-machines/class/08-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"Limits to connectionism","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14, 179—211.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"6fce0ded8aa03d77081bcdf32570ef18","permalink":"/learning-humans-machines/class/09-class/","publishdate":"2020-09-29T00:00:00Z","relpermalink":"/learning-humans-machines/class/09-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14, 179—211.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about?","tags":null,"title":"Recurrent neural networks","type":"docs"},{"authors":null,"categories":null,"content":"   Slides Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601596800,"objectID":"3fea5612523ac7da2f3ba696739f7198","permalink":"/learning-humans-machines/class/10-class/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/learning-humans-machines/class/10-class/","section":"class","summary":"Slides Recording Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?","tags":null,"title":"Bayesian inference","type":"docs"},{"authors":null,"categories":null,"content":"   Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Tenenbaum, J. B. (2000). Rules and similarity in concept learning. In Advances in neural information processing systems (pp. 59—65).\nXu, F., \u0026amp; Tenenbaum, J. B. (2007). Word learning as Bayesian inference. Psychological Review, 114, 245—272..\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"187acf4fd964c835bff8f91f793da537","permalink":"/learning-humans-machines/class/11-class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/class/11-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Tenenbaum, J. B. (2000). Rules and similarity in concept learning. In Advances in neural information processing systems (pp. 59—65).\nXu, F., \u0026amp; Tenenbaum, J. B. (2007). Word learning as Bayesian inference. Psychological Review, 114, 245—272..\n Clearest and most confusing things Go to this form and answer these three questions:","tags":null,"title":"Learning by Bayesian inference","type":"docs"},{"authors":null,"categories":null,"content":"  Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1602288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602288000,"objectID":"df7a31a1291d87ad849bb7c3fb14d771","permalink":"/learning-humans-machines/class/12-class/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/learning-humans-machines/class/12-class/","section":"class","summary":"Recording Clearest and most confusing things   Recording Lecture recording on CMU box.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.","tags":null,"title":"Models at different levels","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Anderson, J. R., \u0026amp; Schooler, L. J. (1991). Reflections of the environment in memory. Psychological Science, 2, 396—408.\nGriffiths, T. L., \u0026amp; Tenenbaum, J. B. (2006). Optimal predictions in everyday cognition. Psychological Science, 17, 767—773.\nKidd, C., Piantadosi, S. T., \u0026amp; Aslin, R. N. (2012). The Goldilocks effect: Human infants allocate attention to visual sequences that are neither too simple nor too complex. PlOS ONE, 7, e36399.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1602547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602547200,"objectID":"32a6d5319519bb5a196fdac45ee2633c","permalink":"/learning-humans-machines/class/13-class/","publishdate":"2020-10-13T00:00:00Z","relpermalink":"/learning-humans-machines/class/13-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Anderson, J. R., \u0026amp; Schooler, L. J. (1991). Reflections of the environment in memory. Psychological Science, 2, 396—408.\nGriffiths, T. L., \u0026amp; Tenenbaum, J. B. (2006). Optimal predictions in everyday cognition. Psychological Science, 17, 767—773.\nKidd, C., Piantadosi, S. T., \u0026amp; Aslin, R.","tags":null,"title":"Rational Analysis","type":"docs"},{"authors":null,"categories":null,"content":"  Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Sanborn, A. N., Griffiths, T. L., \u0026amp; Shiffrin, R. M. (2010). Uncovering mental representations with Markov chain Monte Carlo. Cognitive psychology, 60, 63—106.\nVul, E., Goodman, N., Griffiths, T. L., \u0026amp; Tenenbaum, J. B. (2014). One and done? Optimal decisions from very few samples. Cognitive Science, 38, 599—637.\nVul, E., \u0026amp; Pashler, H. (2008). Measuring the crowd within: Probabilistic representations within individuals. Psychological Science, 19, 645—647.\n Clearest and most confusing things Go to this form and answer these three questions:\nWhat was the most confusing thing from class today? What are you still wondering about? What was the clearest thing from class today? What was the most exciting thing you learned?  I’ll compile the questions and send out answers after class.\n ","date":1602720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602720000,"objectID":"a255e2111ec16f362b9c579591551137","permalink":"/learning-humans-machines/class/14-class/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/learning-humans-machines/class/14-class/","section":"class","summary":"Slides Recording Papers Clearest and most confusing things   Slides Download the slides from today’s class.\n   Recording Lecture recording on CMU box.\n Papers Sanborn, A. N., Griffiths, T. L., \u0026amp; Shiffrin, R. M. (2010). Uncovering mental representations with Markov chain Monte Carlo. Cognitive psychology, 60, 63—106.\nVul, E., Goodman, N., Griffiths, T. L., \u0026amp; Tenenbaum, J. B. (2014). One and done? Optimal decisions from very few samples.","tags":null,"title":"Inference by sampling","type":"docs"},{"authors":null,"categories":null,"content":"  Gershman, S., J. (2015). A unifying probabilistic view of associative learning. PLoS Computational Biology, 11, e1004567.\n You should read the first ~half of the paper until the start of the temporal difference section (bottom of page 7). There’s a great youtube talk-form of this paper from Sam Gershman that is really helpful for understanding the paper. The part for the first half of the paper ends at ~35 minutes. You can watch this instead of or in addition to the reading if you like.\n Your goal should be to understand why this model is different from the Rescorla-Wagner model that you learned about the start of the semester, and be able to talk about their relative merits, and how they relate to higher-level ideas about frameworks for thinking about learning.\n   Gershman, S., J., \u0026amp; Niv, Y. (2012). Exploring a latent cause theory of classical conditioning. Learning \u0026amp; Behavior, 40, 255—268.\n You should understand what motivated this model (relative to classical conditioning models), the phenomena described, and why the model accounts for them.  The primary goal this week is to talk about how this framing of the learning problem is different from prior models of classical conditioning, what it buys us, and how compelling you find this account.\n","date":1603152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"2c7b8d86fe31121096798219ca0a6727","permalink":"/learning-humans-machines/reading/15-reading/","publishdate":"2020-10-20T00:00:00Z","relpermalink":"/learning-humans-machines/reading/15-reading/","section":"reading","summary":"Gershman, S., J. (2015). A unifying probabilistic view of associative learning. PLoS Computational Biology, 11, e1004567.\n You should read the first ~half of the paper until the start of the temporal difference section (bottom of page 7). There’s a great youtube talk-form of this paper from Sam Gershman that is really helpful for understanding the paper. The part for the first half of the paper ends at ~35 minutes.","tags":null,"title":"Bayesian associative learning","type":"docs"},{"authors":null,"categories":null,"content":"   Colunga, E., \u0026amp; Smith, L. B. (2005). From the lexicon to expectations about kinds: a role for associative learning. Psychological Review, 112, 347—382.\n Read the introduction, Experiments 1-3, and the discussion and conclusion. Your goal should be to understand what the phenemon being modeled is, how the model works, and what the basic results are.   Kemp, C., Perfors, A., \u0026amp; Tenenbaum, J. B. (2007). Learning overhypotheses with hierarchical Bayesian models.. Developmental Science, 10, 307—321.\n You can skip the section on ontological kinds. Your goal should again be to understand what the model is doing and why it produces the results it does.  The primary goal this week is to think about the relationship between these two models. How are they the same? How are they different? Are there reasons to prefer one to the other? Are there some things that one does better than the other?\n","date":1602115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602115200,"objectID":"a21d1f023b9fb481f6f6ab263dd324cc","permalink":"/learning-humans-machines/reading/12-reading/","publishdate":"2020-10-08T00:00:00Z","relpermalink":"/learning-humans-machines/reading/12-reading/","section":"reading","summary":"Colunga, E., \u0026amp; Smith, L. B. (2005). From the lexicon to expectations about kinds: a role for associative learning. Psychological Review, 112, 347—382.\n Read the introduction, Experiments 1-3, and the discussion and conclusion. Your goal should be to understand what the phenemon being modeled is, how the model works, and what the basic results are.   Kemp, C., Perfors, A., \u0026amp; Tenenbaum, J. B. (2007). Learning overhypotheses with hierarchical Bayesian models.","tags":null,"title":"Models at different levels","type":"docs"},{"authors":null,"categories":null,"content":"  Marcus, G. F., Vijayan, S., Rao, S. B., \u0026amp; Vishton, P. M. (1999). Rule learning by seven-month-old infants. Science, 283, 77—80. Also read the responses.\n Your goal here should be to understand Marcus et al.’s experimental paradigm and why they think it means that human cognition cannot operate the way that neural networks do. Do you agree? Do you find the criticisms compelling?   McClelland, J. L., \u0026amp; Plaut, D. C. (1999). Does generalization in infant learning implicate abstract algebra-like rules?. Trends in Cognitive Sciences, 3, 166—168. Also read the Marcus response.\n This is a more detailed objection to the Marcus (1999) argument. Make sure you understand what they are suggesting that networks can learn, and also why Marcus is not impressed.  Marcus, G. (2018). Deep learning: A critical appraisal. arXiv preprint.\n Neural networks in 2018 are much more impressive than they were in 1999. And yet, Marcus is still concerned. As you read this, think about whether the same arguments are being made here as in his 1999 paper. Are previous arguments refuted? Are there new compelling arguments?  ","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600905600,"objectID":"84eea7e2ac7daba0422f51df6e252e34","permalink":"/learning-humans-machines/reading/08-reading/","publishdate":"2020-09-24T00:00:00Z","relpermalink":"/learning-humans-machines/reading/08-reading/","section":"reading","summary":"Marcus, G. F., Vijayan, S., Rao, S. B., \u0026amp; Vishton, P. M. (1999). Rule learning by seven-month-old infants. Science, 283, 77—80. Also read the responses.\n Your goal here should be to understand Marcus et al.’s experimental paradigm and why they think it means that human cognition cannot operate the way that neural networks do. Do you agree? Do you find the criticisms compelling?   McClelland, J. L.","tags":null,"title":"Limits to connectionism","type":"docs"},{"authors":null,"categories":null,"content":"  Ramscar, M., Yarlett, D., Dye, M., Denny, K., \u0026amp; Thorpe, K. The Effects of feature-label-order and their implications for symbolic learning. Cognitive Science, 34, 909-957.\n This is a challenging paper—both the theory and the model are pretty dense. If you don’t understand all of the distinctions that the authors are drawing between reference and prediction don’t worry too much about that (section 1). But make sure you understand sections 2-8. The rest of the paper is optional.   Smith, L. B. Learning how to learn words: An associative crane. In R. Golinkoff, \u0026amp; K. Hirsh-Pasek (Eds.), Breaking the word learning barrier (pp. 51-80). Oxford: Oxford University Press.\n This paper should be a little bit easier. Make sure you understand the primary argument about the difference between cranes and skyhooks as explanations, and how that maps onto theories of word learning according to Smith.  ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599770768,"objectID":"13bbf5064f8e7d91a62ab29bec4d6a2c","permalink":"/learning-humans-machines/reading/04-reading/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/learning-humans-machines/reading/04-reading/","section":"reading","summary":"Ramscar, M., Yarlett, D., Dye, M., Denny, K., \u0026amp; Thorpe, K. The Effects of feature-label-order and their implications for symbolic learning. Cognitive Science, 34, 909-957.\n This is a challenging paper—both the theory and the model are pretty dense. If you don’t understand all of the distinctions that the authors are drawing between reference and prediction don’t worry too much about that (section 1). But make sure you understand sections 2-8.","tags":null,"title":"How far can simple associative learning get you?","type":"docs"},{"authors":null,"categories":null,"content":"  Here’s your roadmap for the semester!\n  Readings should be completed before each class session  Assignments are due by 11:59 PM on the day they are due  Class materials (slides, in-class activities, etc.) will be added on the day of class     Simple Neural Networks Reading Assignment Class   September 1  Learning in Humans and Machines        September 3  R, RStudio, and Github        September 8  Associative Learning        September 10  How far can simple associative learning get you?       September 15  Perceptrons        September 16 Implementing the Rescorla-Wagner Model        September 17  Multi-layer Networks        September 22  Backpropagation details        September 24  Limits to connectionism       September 29  Recurrent neural networks        October 9 Perceptrons and backpropagation         Bayesian learning Reading Assignment Class   October 1  Basics of Bayesian Inference        October 6  Learning by Bayesian Inference        October 8  Models at different levels       October 13  Rational analysis        October 15  Inference by sampling        October 20  Bayesian associative learning        October 21 The number game        October 22  Limits on computability         October 27  Top-down vs. bottom-up         October 28 Markov chain monte-carlo          Learning from other people Reading Assignment Class   October 29  Learning from teaching         October 30 Project Proposal         Nov 3  What makes a good teacher?         Nov 5  Rational speech acts         Nov 10  Indirectly learning from language         November 11 Learning and teaching         Nov 12  Iterated learning         Nov 17  Community effects on iterated learning         Nov 19  The structure in language         Nov 24  Modern language models         November 25 Latent semantic analysis         Nov 26 Thanksgiving - No class         Dec 1  Why training data matters          Projects Reading Assignment Class   Dec 3  Project Presentations         Dec 10  Project Presentations         Dec 8  Wrap up         December 15 Final Project due          ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599770768,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"/learning-humans-machines/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/schedule/","section":"","summary":"Here’s your roadmap for the semester!\n  Readings should be completed before each class session  Assignments are due by 11:59 PM on the day they are due  Class materials (slides, in-class activities, etc.) will be added on the day of class     Simple Neural Networks Reading Assignment Class   September 1  Learning in Humans and Machines        September 3  R, RStudio, and Github        September 8  Associative Learning        September 10  How far can simple associative learning get you?","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"  Course objectives Important pep talk! Course structure Seminar Commentaries Homeworks Final Project  Course materials R and RStudio Github Online help  Course policies Course evaluation and evolution Office hours Accommodations for Students with Disabilities: Respect in the Classroom Health \u0026amp; Well-Being Academic honesty  Assignments and grades   Instructor  Dr. Dan Yurovsky  Zoom Office  Tuesday 4:30-6:40 PM  yurovsky@cmu.edu  @danyurovsky   Course details  Tuesday/Thursday  September 15–April 22, 2020  1:30–2:50 PM  Zoom Room   Contacting me E-mail is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours (really), but also remember that life can be busy and chaotic for everyone (including me!), so if I don\u0026rsquo;t respond to your e-mail right away, don\u0026rsquo;t worry!\n  Course objectives By the end of this course, you will (1) have an understanding of formal frameworks used to characterize human learning, (2) understand how these are related to methods used in machine learning, (3) be able to implement these theories in working code, and (4) be able to apply these methods to new problems.\nSpecifically, you’ll be able to:\nArticulate a formal definition of what it means to “learn,” and why this can be different across different settings Describe (some) of what is known empirically about how humans learn Implement formal models of human learning from several different frameworks in the R programming language Explain how these models relate to models developed in machine learning Characterize how “good” a model is and the ways in which it is better or worse than other models of the same phenomenon.   Important pep talk! I promise you can succeed in this class.\nLearning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2—made this wise observation:\n It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n Even experienced programmers and evaluators find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n Course structure Because the goal for the course is for you to both learn theory and be able to apply it in practice, you’ll be learning through three primary modes: (1) short lectures, (2) hands-on programming assignments, and (3) seminar discussions with your classmates.\nThroughout each of the units, I will present lectures to familiarize you with the broad strokes of the important ideas, and to scaffold your ability to learn from the papers you will read and the assignments you will complete. I will generally lecture more at the start of the course than at the end. All lectures will be available online after class is finished.\nSeminar For the first unit, I will lead the seminar discussions that are interspersed with the lectures in order to set the tone for the course and to establish my expectations for the goals of seminar and present one way of trying to accomplish them. After that, students will be asked to lead the seminars in pairs or small groups. Before you lead a seminar, I’ll meet with you help you plan a successful discussion.\nRegardless of whether you are leading discussion, I expect everyone in the class to be an active participant–you’ll have a lot more fun, and learn a whole lot more if you engage with your classmates’ ideas while we are all together. I know that this is hard to do, especially in the context of a remote class, and that you may be subject to circumstances that make it particularly challenging. For this reason, we will also have a Piazza forum where you can join in the discussion asynchronously. If you are unable to attend seminar, please make sure you contribute significantly here.\n Commentaries To help seed seminar discussion, you will be asked to submit a short commentary to the Piazza forum. Please submit your commentary by midnight day before seminar\nA commentary might take one of the following forms:\nDisagreement. Mention a claim that doesn’t seem right to you.\n Extension. Describe how the work could be usefully extended. Is there another way that the model could be tested? How might the model be revised to handle a broader range of cognitive phenomena?\n Criticism. List at least one flaw or limitation with the approach presented.\n Connection. Draw a connection between the reading and something else that you know about (e.g. something that we’ve discussed in a previous class, or that you learned about elsewhere).\n  Each commentary should not be long—one paragraph is typical. You can write up to half a page, but writing three thoughtful sentences is sufficient.\nThe presenters for the day will be reading your commentaries before class, so feel free to add a postscript with questions or thoughts about what you’d like to discuss during class. For example, if the reading made some point that was confusing but that you’d like to understand, include a note to this effect and we’ll try to address it during class.\n Homeworks Each unit will have a two hands-on programming assignments where you will implement the theoretical ideas we are discussing. The best way to develop an intuition for how models work and what they can (and can’t) do is to build them yourself. You are welcome to work on these together with other students, but you must turn in your own work\n Final Project As a capstone for the class, you will complete a final project. Your project can take one of several forms. You can develop a new model of some aspect of cognition, run a small experiment to test an existing computational model, implement an existing model and apply it to a new data set, or write a paper on some topic related to the class. Working on an existing research project is fine as long as this project matches the spirit of the class. You are encouraged to work in pairs, but working on your own is also fine.\nThe project includes four components:\nProposal. Submit no more than one page describing the question you plan to explore and the method you will use. Your proposal is due on October 30.\n Check in. You will meet with me during the week of November 9 to discuss how your project is going.\n Presentation. At the end of the semester you will briefly present your project. Your presentation should be about 10 minutes long, and you are encouraged to use visual aids (e.g. slides, figures drawn on the zoom board).\n Writeup.\n    Course materials All of the materials you will need for this course–both the readings and the software will be made freely available to you. Any articles, book chapters, or other materials I will ask you to read will be linked on the course website from the related week. Each will be accompanied by a short reading guide that will help you to know what to focus on when you read.\nR and RStudio You will do all of your programming in the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all of the computation, while RStudio provides a nice interface for running R code.\n Github In order to access and submit the assignments for the course, we will use GitHub Classroom. GitHub is a cloud-based version control system that is extremely popular in both academia and industry. You will likely find it more challenging to use at first than canvas, but you’ll learn skills that will generalize beyond the course.\n Online help Programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nIn addition to asking me and your classmates for help, there are tons of online resources to help you with this. One of the most important is StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions).\n  Course policies Be nice. Be honest. Try your best. Don’t cheat.\nWe will also follow the Carnegie Mellon University code of conduct.\nCourse evaluation and evolution This syllabus reflects a plan for the semester. Deviations may become necessary as the semester progresses.\nBecause this is a new class, there will be inevitable bumps along the way. Please be patient with me as I get everything ironed out!\nI want to make this the best class I can, and I’d love your help to help to do that. To facilitate this, I have a couple requests:\nIn the class link next to every class, you will find a link to an anonymous Google Form with a few quick questions asking about the clearest and most confusing things from that day. Please fill this out regularly. It will be hard to remember, but it’s extraordinarily helpful for me.\n At some point in the middle of the semester, someone from CMU’s Eberly Center for Teaching Excellence \u0026amp; Educational Innovation will come and run a 25 minute Early course feedback focus group, where I’ll step out and you’ll all talk to them about the class. They’ll give me anonymized feedback about what is going well, and what I should do to help you learn better.\n  Also, please take the time to fill out the official CMU course evaluation at the end of the semester!\n Office hours Please watch this video:\n Office hours are set times dedicated to all of you. This means that I will be sitting on Zoom (wistfully) waiting for you to come by with whatever questions you have. This is the best and easiest way to find me outside of class and the best chance for discussing class material and concerns. Please come by!\nOutside of regularly scheduled office hours, send me an email and I will be very happy to meet with you.\nThis can be a difficult class. Do not suffer in silence! Come talk to me!\n Accommodations for Students with Disabilities: If you have a disability and are registered with the Office of Disability Resources, I encourage you to use their online system to notify me of your accommodations and discuss your needs with me as early in the semester as possible. I will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, I encourage you to contact them.\n Respect in the Classroom It is my intent to present materials and activities that are respectful to the diverse backgrounds and perspectives of students in the classroom. You may feel free to let me know ways to improve the effectiveness of the course for you personally or for other students or student groups. If you feel uncomfortable discussing this with your instructor, you may voice your concerns to the Chair of the Department of Psychology Diversity and Inclusion (D\u0026amp;I) Committee, Jessica Cantlon. Dr. Cantlon and the D\u0026amp;I Committee are available to hear your concerns related to respect for diversity for any class you are taking in the Department of Psychology.\n Health \u0026amp; Well-Being Take care of yourself. Maintaining a healthy lifestyle via good nutrition, exercise, rest, and relaxation will help you achieve your goals and cope with stress. All of us benefit from support during times of struggle. There are many helpful resources available, so please reach out to us if you need help connecting with them on campus or virtually. Asking for support sooner rather than later is almost always helpful. If you or anyone you know experiences academic stress, difficult life events, or feelings of anxiety or depression, please seek support. Counseling and Psychological Services (CaPS) is available online and at 412-268-2922 or via. Consider reaching out to a friend, faculty member, or family member you trust for assistance with getting connected to the support that can help.\n Academic honesty Cheating and plagiarism are defined in the CMU Student Handbook, and include (1) submitting work that is not your own for assignments or exams; (2) copying ideas, words, or graphics from a published or unpublished source without appropriate citation; (3) submitting or using falsified data; and (4) submitting the same work for credit in two courses without prior consent of both instructors. Any student who is found cheating or plagiarizing on any work for this course will receive a failing grade for that work. Further action may be taken if necessary, including a report to the dean.\n  Assignments and grades Descriptions for all of the assignments will be posted on the on the assignments page.\n   Assignment Points Percent    Homeworks (6 x 12) 72 49%  Participation 15 10%  Commentaries 15 10%  Project presentation 15 10%  Final project 30 20%  Total 147 —        Grade Range Grade Range    A 93–100% C 73–76%  A− 90–92% C− 70–72%  B+ 87–89% D+ 67–69%  B 83–86% D 63–66%  B− 80–82% D− 60–62%  C+ 77–79% F \u0026lt; 60%      ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599770768,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"/learning-humans-machines/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-humans-machines/syllabus/","section":"","summary":"Course objectives Important pep talk! Course structure Seminar Commentaries Homeworks Final Project  Course materials R and RStudio Github Online help  Course policies Course evaluation and evolution Office hours Accommodations for Students with Disabilities: Respect in the Classroom Health \u0026amp; Well-Being Academic honesty  Assignments and grades   Instructor  Dr. Dan Yurovsky  Zoom Office  Tuesday 4:30-6:40 PM  yurovsky@cmu.edu  @danyurovsky   Course details  Tuesday/Thursday  September 15–April 22, 2020  1:30–2:50 PM  Zoom Room   Contacting me E-mail is the best way to get in contact with me.","tags":null,"title":"Syllabus","type":"page"}]